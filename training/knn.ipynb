{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SydCnitAeLO_"},"outputs":[],"source":["%%capture\n","import pandas as pd\n","import numpy as np\n","import warnings\n","from google.colab import drive\n","import ipaddress\n","\n","warnings.filterwarnings('ignore')\n","drive.mount('/content/drive')\n","\n","FILEPATH = \"/content/drive/MyDrive/data analytics/knn_new/results_0.csv\""]},{"cell_type":"markdown","metadata":{"id":"oK4AXarlSXJf"},"source":["# Creazione df result"]},{"cell_type":"code","source":["def create_empty_df(filepath):\n","  \"\"\"\n","  Crea un DataFrame vuoto e lo salva in un file CSV.\n","  \"\"\"\n","  # Definisci le colonne del DataFrame\n","  columns = ['ds', 'random', 'outlier', 'dim_reduction', 'pca_threshold', 'scaler', 'n_neighbors', 'metric', 'weights', 'target count','info']\n","\n","  # Crea un DataFrame vuoto\n","  results_df = pd.DataFrame(columns=columns)\n","\n","  # Salva il DataFrame in formato CSV\n","  results_df.to_csv(filepath, index=False)\n","\n","  print(f\"DataFrame creato e salvato in {filepath}\")\n","\n","create_empty_df(FILEPATH)"],"metadata":{"id":"_PR2nWm7RUOd","executionInfo":{"status":"ok","timestamp":1738497250417,"user_tz":-60,"elapsed":326,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ceda391d-3451-4dc2-c7b5-6aafb43880da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame creato e salvato in /content/drive/MyDrive/data analytics/knn_new/results_0.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"FJgi_sXUxr9c"},"source":["# Data Cleaning\n","Scelta colonne, cast delle colonne e gestione dei valori nulli\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05w29vRUj2Q9"},"outputs":[],"source":["# Funzione per determinare il tipo di dato di una colonna\n","def type_data(column):\n","    default_val = [np.nan, '-']\n","    column = column[~column.isin(default_val)]\n","    unique_count = column.nunique()\n","    if is_binary_dtype(column):\n","        return 'Binario'\n","    if  is_numeric_dtype(column):\n","        return 'Numerico Discreto' if pd.api.types.is_integer_dtype(column) else 'Numerico Continuo'\n","    if is_category_dtype(column):\n","        return 'Categorico'\n","    return 'Unknown'\n","\n","# Funzioni ausiliarie per verificare il tipo di dato\n","def is_numeric_dtype(column):\n","    return pd.api.types.is_numeric_dtype(column)\n","\n","def is_binary_dtype(column):\n","    return set(column.unique()) == {True, False}\n","\n","def is_category_dtype(column):\n","    return pd.api.types.is_object_dtype(column) or pd.api.types.is_categorical_dtype(column)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-_JHS8nz5Uv"},"outputs":[],"source":["def clean_service_columns(data):\n","    service_related_cols = {}\n","    categorial_columns = data.select_dtypes(exclude=np.number).columns\n","    for col in categorial_columns:\n","      for prefix in ['dns', 'http', 'ssl']:\n","        if col.startswith(prefix) and not pd.api.types.is_numeric_dtype(col):\n","          if prefix not in service_related_cols:\n","            service_related_cols[prefix] = []\n","          service_related_cols[prefix].append(col)\n","    for col in data.columns:\n","        for service, columns in service_related_cols.items():\n","            if col in columns and f\"service_{service}\" in data.columns:\n","                data.loc[~data[f\"service_{service}\"], col] = '/'\n","    return data\n","\n","def boolean_mapping(value, def_val=None):\n","    if value in {True, False}:\n","        return value\n","    if value == 'T':\n","        return True\n","    if value == 'F':\n","        return False\n","    return def_val if def_val is not None else value\n","\n","def categorize_ports(df, port_columns):\n","    port_bins = [0, 1023, 49151, 65535]\n","    port_labels = [\"Well-Known\", \"Registered\", \"Dynamic\"]\n","    for col in port_columns:\n","        df[col] = pd.cut(df[col], bins=port_bins, labels=port_labels, right=True)\n","    return df\n","\n","\n","def categorize_ip(ip):\n","    try:\n","        ip_obj = ipaddress.ip_address(ip)\n","        if ip_obj.is_loopback:\n","            return \"Loopback\"\n","        if ip_obj.is_private:\n","            return \"Private\"\n","        if ip_obj.is_multicast:\n","            return \"Multicast\"\n","        if ip_obj.is_reserved:\n","            return \"Reserved\"\n","        if ip_obj.is_link_local:\n","            return \"Link-Local\"\n","        return \"Public\"\n","    except ValueError:\n","        return \"Invalid\"\n","\n","def df_mapping(df):\n","  rcode_mapping = {0: 'No Error', 2: 'ServerFailure', 3: 'NameError', 5: 'Refuse'}\n","  qclass_mapping = {0: '-', 1: 'IN', 32769: 'CH'}\n","  qtype_mapping = {0: '-', 1: 'A', 2: 'NS', 5: 'CNAME', 28: 'AAAA', 255: 'ANY'}\n","\n","  for col in df.columns:\n","    if col in ['dns_RD', 'dns_RA', 'dns_AA', 'dns_rejected', 'ssl_established', 'ssl_resumed', 'weird_notice']:\n","      df[col] = df[col].map(lambda x: boolean_mapping(x)).astype(str)\n","    if col in ['http_status_code', 'weird_addl', 'http_trans_depth']:\n","      df[col] = df[col].astype(str)\n","    if col == 'dns_qclass':\n","      df[col] = df[col].apply(lambda x: qclass_mapping.get(x, None))\n","    if col == 'dns_qtype':\n","      df[col] = df[col].apply(lambda x: qtype_mapping.get(x, None))\n","    if col == 'dns_rcode':\n","      df[col] = df[col].apply(lambda x: rcode_mapping.get(x, None))\n","    if col in ['src_ip', 'dst_ip']:\n","      df[col] = df[col].apply(categorize_ip)\n","    if col == 'src_bytes':\n","      df = df[df['src_bytes'] != '0.0.0.0']\n","      df['src_bytes'] = df['src_bytes'].astype(int)\n","  df = categorize_ports(df, ['src_port', 'dst_port'])\n","  return df\n","\n","def data_cleaning(df):\n","    services = df['service'].str.split(';').explode().unique()  # Estrazione di tutti i servizi unici\n","    for service in services:\n","        df[f'service_{service}'] = df['service'].apply(lambda x: service in x.split(';'))\n","\n","    df.drop(['http_referrer', 'service', 'service_-'], axis=1, inplace=True, errors='ignore')\n","    df.drop(['ts', 'ssl_version', 'ssl_resumed', 'ssl_established', 'ssl_subject', 'ssl_issuer', 'dns_query',\n","                  'dns_qclass', 'dns_qtype', 'dns_rcode', 'dns_RD', 'dns_AA', 'dns_rejected', 'http_method', 'http_uri',\n","                  'http_version', 'http_orig_mime_types', 'http_resp_mime_types', 'http_request_body_len',\n","                  'http_response_body_len', 'http_user_agent', 'weird_name', 'weird_addl', 'weird_notice', 'label'],\n","             axis=1, inplace=True, errors='ignore')\n","\n","    df = df_mapping(df)\n","    df = clean_service_columns(df)\n","\n","    return df"]},{"cell_type":"code","source":["def replace_default_new(df, info):\n","\n","    mode_values = {}\n","    if info == 'mode' or info=='mode_all':\n","      for col in df.columns:\n","        if is_category_dtype(df[col]) or is_binary_dtype(df[col]):\n","            valid_values = df[(df[col] != '/') & (df[col] != '-')][col]\n","            mode_value = valid_values.mode()[0] if not valid_values.empty else '-'  # Usa '-' se non c'è moda\n","            mode_values[col] = mode_value\n","\n","            # Sostituzione valori\n","            df[col] = df[col].replace('-', mode_value)\n","            if info == 'mode_all':\n","              df[col] = df[col].replace('/', mode_value)\n","\n","    # Salva le mode con joblib\n","    joblib.dump(mode_values, \"mode.pk\")\n","\n","    return df\n","\n","\n","def apply_saved_modes(val, info):\n","\n","    if info == 'mode' or info=='mode_all':\n","      mode_values = joblib.load(\"mode.pk\")\n","      # Applica le mode ai nuovi dati\n","      for col, mode_value in mode_values.items():\n","          if col in val.columns:\n","              val[col] = val[col].replace('-', mode_value)\n","              if info == 'mode_all':\n","                val[col] = val[col].replace('/', mode_value)\n","    return val"],"metadata":{"id":"qYX3qCVdyIyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7d7wavGkHeZ"},"outputs":[],"source":["df=pd.read_csv('/content/drive/MyDrive/data analytics/train_dataset.csv')\n","df = data_cleaning(df)"]},{"cell_type":"markdown","metadata":{"id":"EprNSrtG020W"},"source":["# Divisione val e train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijcFM87B01a8"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Definisci le features (X) e il target (y)\n","X = df.drop('type', axis=1)  # Assumi che 'label' sia la colonna del target\n","y = df['type']\n","\n","# Dividi il dataset in train e test set\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=19)\n","\n","# Unisci X_train e y_train\n","train_df = pd.concat([X_train, y_train], axis=1)\n","\n","# Unisci X_test e y_test\n","test_df = pd.concat([X_val, y_val], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"xuYxMmVa9Yyh"},"source":["# Pipline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yfSMEUnyn3Q"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, LabelEncoder, Normalizer\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.decomposition import PCA\n","from sklearn.utils import shuffle\n","import joblib\n","from imblearn.over_sampling import SMOTE,  BorderlineSMOTE, RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYW2GAR10g9D"},"outputs":[],"source":["#rimozione outlier per classe\n","def remove_outliers(x, y, out):\n","    x_train = x.copy()\n","    y_train = y.copy()\n","\n","    df = pd.concat([x_train, y_train], axis=1)\n","    numeric_cols = x_train.select_dtypes(include=np.number).columns\n","\n","    # Controlla se non deve essere applicata nessuna rimozione\n","    if out == 'no':\n","        return x, y\n","\n","    if out == 'base':\n","      #rimozione outlier piÃ¹ ASSURDI\n","      before = df.shape[0]\n","      df = df[df['duration'] < 1000]\n","      df = df[df['src_bytes']<100000000]\n","      df = df[df['dst_bytes']<100000000]\n","      df = df[df['missed_bytes']<100000000]\n","      df = df[df['src_pkts']<20000]\n","      df = df[df['dst_pkts']<20000]\n","      df = df[df['src_ip_bytes']<1000000]\n","      df = df[df['dst_ip_bytes']<1000000]\n","      print('  Rimosse ',before-df.shape[0],' istanze')\n","      x_train = df.drop('type', axis=1)\n","      y_train = df['type']\n","      return x_train, y_train\n","\n","    filtered_data = []\n","    # Itera su ciascuna classe\n","    for cls in df['type'].unique():\n","        class_df = df[df['type'] == cls]\n","        before = class_df.shape[0]\n","\n","        if out == 'iqr':\n","            for col in numeric_cols:\n","                Q1 = class_df[col].quantile(0.25)\n","                Q3 = class_df[col].quantile(0.75)\n","                IQR = Q3 - Q1\n","                lower_bound = Q1 - 1.5 * IQR\n","                upper_bound = Q3 + 1.5 * IQR\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        elif out == 'percentile':\n","            for col in numeric_cols:\n","                lower_bound = class_df[col].quantile(0.01)\n","                upper_bound = class_df[col].quantile(0.99)\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        elif out == 'isolation_forest':\n","            from sklearn.ensemble import IsolationForest\n","            iso = IsolationForest(contamination=0.05, random_state=19)\n","            numeric_data = class_df[numeric_cols]\n","            class_df['outlier'] = iso.fit_predict(numeric_data)\n","            class_df = class_df[class_df['outlier'] == 1].drop(columns=['outlier'])\n","\n","        elif out == 'dynamic_threshold':\n","            for col in numeric_cols:\n","                mean = class_df[col].mean()\n","                std = class_df[col].std()\n","                lower_bound = mean - 3 * std\n","                upper_bound = mean + 3 * std\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        filtered_data.append(class_df)\n","\n","    # Combina i dati filtrati per ciascuna classe\n","    filtered_df = pd.concat(filtered_data)\n","\n","    x_train = filtered_df.drop('type', axis=1)\n","    y_train = filtered_df['type']\n","\n","    return x_train, y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1jZXJG_ysK9"},"outputs":[],"source":["# scaling and normalization\n","def scale_train_data(x_train, y_train, scaling_method):\n","    scaled_df = x_train.copy()\n","\n","    numeric_columns = x_train.select_dtypes(include=np.number).columns\n","    if len(numeric_columns) == 0:\n","        print(\"  Warning: No numeric columns to scale. Returning original DataFrame.\")\n","        return scaled_df, y_train\n","\n","    if scaling_method == 'none':\n","        print(\"No scaling applied.\")\n","        return scaled_df, y_train\n","    elif scaling_method == 'standard':\n","        scaler = StandardScaler()\n","    elif scaling_method == 'minmax':\n","        scaler = MinMaxScaler()\n","    elif scaling_method == 'quantile':\n","        scaled_df = pd.concat([scaled_df, y_train], axis=1)\n","        scaled_df = scaled_df.sort_values(by='src_bytes')\n","        y_train = scaled_df['type']\n","        scaled_df = scaled_df.drop('type', axis=1)\n","        scaler = QuantileTransformer(output_distribution='uniform', random_state=19)\n","    elif scaling_method == 'l1':\n","        scaler = Normalizer(norm='l1')\n","    elif scaling_method == 'l2':\n","        scaler = Normalizer(norm='l2')\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    if scaled_df[numeric_columns].shape[0] < 1:\n","        print(\"  Warning: Not enough samples to fit the scaler. Returning original DataFrame.\")\n","        return scaled_df, y_train\n","\n","    if scaling_method == 'l1' or scaling_method == 'l2':\n","        scaled_df = scaler.fit_transform(scaled_df)\n","    else:\n","        scaled_df[numeric_columns] = scaler.fit_transform(scaled_df[numeric_columns])\n","    joblib.dump(scaler, \"scaler.pkl\")\n","    return scaled_df, y_train\n","\n","# carica scaler e effettua scaling\n","def scale_validation_data(x_val, y_val, scaling_method):\n","    if scaling_method == 'quantile':\n","        x_val = pd.concat([x_val, y_val], axis=1)\n","        x_val = x_val.sort_values(by='src_bytes')\n","        y_val = x_val['type']\n","        x_val = x_val.drop('type', axis=1)\n","\n","    numeric_columns = x_val.select_dtypes(include=np.number).columns\n","    if scaling_method == 'none':\n","        print(\"No scaling applied to validation data.\")\n","        return x_val, y_val\n","\n","    scaler = joblib.load(\"scaler.pkl\")\n","\n","    if scaling_method == 'l1' or scaling_method == 'l2':\n","        x_val = scaler.transform(x_val)\n","    else:\n","        x_val[numeric_columns] = scaler.transform(x_val[numeric_columns])\n","    return x_val, y_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92oPrwzabh3i"},"outputs":[],"source":["# ENCODING\n","def encode_categorical_train_data(x_train):\n","    categorical_columns = x_train.select_dtypes(include=['object', 'category']).columns\n","\n","    if len(categorical_columns) > 0:\n","        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n","        encoder.fit(x_train[categorical_columns])\n","        joblib.dump(encoder, \"onehot_encoder.pkl\")\n","        x_train_encoded = encoder.transform(x_train[categorical_columns])\n","        encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n","        x_train_encoded_df = pd.DataFrame(x_train_encoded, columns=encoded_feature_names, index=x_train.index)\n","        x_train = x_train.drop(columns=categorical_columns)\n","        x_train = pd.concat([x_train, x_train_encoded_df], axis=1)\n","\n","    return x_train\n","\n","def encode_categorical_validation_data(x_val):\n","    categorical_columns = x_val.select_dtypes(include=['object', 'category']).columns\n","    encoder = joblib.load(\"onehot_encoder.pkl\")\n","\n","    if len(categorical_columns) > 0:\n","        x_val_encoded = encoder.transform(x_val[categorical_columns])\n","        encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n","        x_val_encoded_df = pd.DataFrame(x_val_encoded, columns=encoded_feature_names, index=x_val.index)\n","        x_val = x_val.drop(columns=categorical_columns)\n","        x_val = pd.concat([x_val, x_val_encoded_df], axis=1)\n","\n","    return x_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbCAdHq90VFz"},"outputs":[],"source":["# BILANCIAMENTO\n","def balance_data(x_train, y_train, target_count, num_datasets, random_seed):\n","    smote = SMOTE(random_state=random_seed)\n","    oversampler = RandomOverSampler(random_state=random_seed)\n","\n","    class_counts = pd.Series(y_train).value_counts()\n","    smote_classes = [cls for cls in class_counts.index if class_counts[cls] < target_count / 2]\n","\n","    if smote_classes:\n","        smote_strategy = {cls: target_count for cls in smote_classes}\n","        smote = SMOTE(sampling_strategy=smote_strategy, random_state=random_seed)\n","        x_train, y_train = smote.fit_resample(x_train, y_train)\n","        class_counts = pd.Series(y_train).value_counts()\n","\n","    over_classes = [cls for cls in class_counts.index if class_counts[cls] < target_count]\n","    if over_classes:\n","        over_strategy = {cls: target_count for cls in over_classes}\n","        oversampler = RandomOverSampler(sampling_strategy=over_strategy, random_state=random_seed)\n","        x_train, y_train = oversampler.fit_resample(x_train, y_train)\n","\n","    datasets = []\n","    for i in range(num_datasets):\n","        undersampler = RandomUnderSampler(sampling_strategy={cls: target_count for cls in pd.Series(y_train).value_counts().index}, random_state=random_seed + i)\n","        x_resampled, y_resampled = undersampler.fit_resample(x_train, y_train)\n","        x_resampled, y_resampled = shuffle(x_resampled, y_resampled, random_state=random_seed + i)\n","        datasets.append((x_resampled, y_resampled))\n","\n","\n","\n","    return datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Atst5e5-0c_H"},"outputs":[],"source":["# PCA\n","def apply_pca_train(x_train, random_state, pca_threshold=0.99):\n","    pca = PCA(random_state=random_state)\n","    pca.fit(x_train)\n","    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n","    n_components = (cumulative_variance >= pca_threshold).argmax() + 1\n","    pca = PCA(n_components=n_components, random_state=random_state)\n","    transformed_data = pca.fit_transform(x_train)\n","    transformed_data = transformed_data.astype(np.float32)\n","\n","    print(f\"  Numero di colonne selezionate (componenti principali): {n_components}\")\n","    joblib.dump(pca, \"pca_model.pkl\")\n","    return pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n","\n","def apply_pca_validation(x_val):\n","    pca = joblib.load(\"pca_model.pkl\")\n","    x_val = pca.transform(x_val)\n","    x_val = x_val.astype(np.float32)\n","    return x_val\n","\n","# LDA\n","def apply_lda_train(x_train, y_train, lda_components=None):\n","    lda = LDA(n_components=lda_components)\n","    lda.fit(x_train, y_train)\n","    transformed_data = lda.transform(x_train)\n","    transformed_data = transformed_data.astype(np.float32)\n","\n","    n_components = transformed_data.shape[1]\n","    print(f\"  Numero di colonne selezionate (componenti discriminanti): {n_components}\")\n","    joblib.dump(lda, \"lda_model.pkl\")\n","    return pd.DataFrame(transformed_data, columns=[f\"LD{i+1}\" for i in range(n_components)])\n","\n","def apply_lda_validation(x_val):\n","    lda = joblib.load(\"lda_model.pkl\")\n","    x_val = lda.transform(x_val)\n","    x_val = x_val.astype(np.float32)\n","    return x_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YueC9OPp9afg"},"outputs":[],"source":["def preprocessing_pipeline(x_train, y_train, x_validation, y_validation, scaling_method, use_pca, pca_threshold, target_count=20000, num_datasets=1, random_seed=19):\n","    # Encoding delle feature\n","    x_train = encode_categorical_train_data(x_train)\n","    x_validation = encode_categorical_validation_data(x_validation)\n","\n","    # Bilanciamento\n","    datasets = balance_data(x_train, y_train, target_count, num_datasets, random_seed)\n","    validation = []\n","    data = []\n","    i = 0\n","\n","    for x_train, y_train in datasets:\n","      print(f\"  Dataset bilanciato {i+1}:\")\n","      i+=1\n","      # Scaling\n","      x_train, y_train = scale_train_data(x_train, y_train, scaling_method)\n","      x_val, y_val = scale_validation_data(x_validation, y_validation, scaling_method)\n","\n","      if use_pca == 'PCA':\n","          x_train = apply_pca_train(x_train, random_state=random_seed, pca_threshold=pca_threshold)\n","          x_val = apply_pca_validation(x_val)\n","      elif use_pca == 'LDA':\n","          x_train = apply_lda_train(x_train, y_train)\n","          x_val = apply_lda_validation(x_val)\n","\n","      # Bilanciamento\n","      data.append((x_train, y_train))\n","      validation.append((x_val, y_val))\n","    return data, validation\n"]},{"cell_type":"markdown","metadata":{"id":"lTUGMeKlHm1S"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jez8IsOIKtHp"},"outputs":[],"source":["import os\n","\n","# Verifica se una combinazione è già presente nel file CSV.\n","def is_combination_tested(filepath, new_row):\n","  existing_results = pd.read_csv(filepath)\n","  # Colonne per la comparazione\n","  comparison_columns = ['ds', 'random', 'outlier', 'dim_reduction', 'pca_threshold', 'scaler', 'n_neighbors', 'metric', 'weights', 'target count', 'info']\n","  new_row['pca_threshold'] = str(new_row['pca_threshold'])\n","  # Verifica se la combinazione esiste già\n","  is_tested = ((existing_results[comparison_columns] == pd.DataFrame([new_row])[comparison_columns].iloc[0]).all(axis=1).any())\n","  if is_tested:\n","      print(\"  Configurazione già testata, salto...\")\n","  return is_tested\n","\n","def append_and_save_results(filepath, new_row):\n","  results_df = pd.read_csv(filepath)\n","  results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n","  results_df.to_csv(filepath, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pg2hqus-HmOT"},"outputs":[],"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","from itertools import product\n","\n","\n","def evaluate_model(model, x_train, y_train, x_val, y_val):\n","  # Predizioni e metriche sul validation set\n","  y_pred_val = model.predict(x_val)\n","  score_val = accuracy_score(y_val, y_pred_val)\n","  report = classification_report(y_val, y_pred_val, output_dict=True)\n","\n","  # Predizioni e metriche sul training set\n","  y_pred_train = model.predict(x_train)\n","  train_score = accuracy_score(y_train, y_pred_train)\n","\n","  print(f\" Validation score: {score_val} - Train score {train_score}\")\n","  return {'validation_score': score_val, 'train_score': train_score, 'classification_report': report}\n","\n","\n","def train_knn_with_grid(x_train, y_train, x_val, y_val, param_grid, metadata, random_state=19):\n","  \"\"\"\n","  Cerca i migliori iperparametri di un KNN valutando direttamente sul validation set.\n","  Salta configurazioni già testate.\n","  \"\"\"\n","  keys, values = zip(*param_grid.items())\n","  param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n","\n","  best_score = -float('inf')\n","  best_model = None\n","  best_report = None\n","\n","  for params in param_combinations:\n","      print(f\"Valutando configurazione: {params}\")\n","\n","      # Crea una nuova riga di metadata\n","      new_row = metadata.copy()\n","      new_row.update(params)\n","\n","      # Verifica se la configurazione è già stata testata\n","      if not is_combination_tested(FILEPATH, new_row):\n","          # Addestra il modello\n","          model = KNeighborsClassifier(**params, n_jobs=-1)\n","          model.fit(x_train, y_train)\n","\n","          # Valutazione\n","          results = evaluate_model(model, x_train, y_train, x_val, y_val)\n","          new_row.update({\n","              'accuracy': results['classification_report']['accuracy'],\n","              'precision': results['classification_report']['weighted avg']['precision'],\n","              'recall': results['classification_report']['weighted avg']['recall'],\n","              'f1': results['classification_report']['weighted avg']['f1-score'],\n","              'train_score': results['train_score']\n","          })\n","\n","          # Salva i risultati\n","          append_and_save_results(FILEPATH, new_row)\n","\n","          # Aggiorna il migliore modello\n","          if results['validation_score'] > best_score:\n","              best_score = results['validation_score']\n","              best_report = results['classification_report']\n","              best_model = model\n","\n","  return best_model, best_score, best_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7O71MA0xtwq_"},"outputs":[],"source":["def apply(x_train, y_train, x_val, y_val, scaling_methods, param_grid,\n","  dim_reduction=['no'], pca_threshold=0.99, target_count=20000, num_datasets=1,\n","  random_seed=19, outs=['no'], info=''):\n","  \"\"\"\n","  Esegue l'intera pipeline di preprocessing, training e valutazione.\n","  \"\"\"\n","  results = {}\n","  x_train_fix = x_train\n","  x_val_fix = x_val\n","  bestbest = 0\n","\n","  for dim_redx in dim_reduction:\n","    print(f\"\\n=== Testing Dimensionality Reduction: {dim_redx} ===\")\n","    if dim_redx != 'PCA':\n","      pca_threshold = None\n","    for scaling_method in scaling_methods:\n","      for out in outs:\n","          print(f\"\\n=== Testing Scaling Method: {scaling_method}, Outlier: {out} ===\")\n","\n","          x_train = replace_default_new(x_train_fix.copy(), info)\n","          x_val = apply_saved_modes(x_val_fix.copy(), info)\n","\n","          # Rimuovi outlier\n","          x_train_filtered, y_train_filtered = remove_outliers(x_train, y_train, out)\n","\n","          # Preprocessing\n","          datasets, validation = preprocessing_pipeline(x_train_filtered, y_train_filtered, x_val, y_val,\n","              scaling_method, dim_redx, pca_threshold, target_count, num_datasets, random_seed\n","          )\n","\n","          results = {}\n","          for i, (x_train_processed, y_train_processed) in enumerate(datasets):\n","              x_val_processed, y_val_processed = validation[i]\n","              ds_name = f\"dataset_{i+1}\"\n","              metadata = {\n","                  'ds': ds_name,\n","                  'random': random_seed,\n","                  'outlier': out,\n","                  'dim_reduction': dim_redx,\n","                  'pca_threshold': pca_threshold,\n","                  'scaler': scaling_method,\n","                  'target count': target_count,\n","                  'info': info\n","              }\n","\n","              print(f\"--- Training Dataset {i+1}/{len(datasets)} ---\")\n","              best_model, best_score, best_report = train_knn_with_grid(\n","                  x_train_processed, y_train_processed,\n","                  x_val_processed, y_val_processed,\n","                  param_grid, metadata, random_seed\n","              )\n","              if best_score > bestbest:\n","                bestbest = best_score\n","                joblib.dump(best_model, \"best_model.pkl\")\n","                print(best_score)\n","                print(best_report)\n","              if best_model:\n","                  results[f\"{scaling_method}_dataset_{i+1}\"] = {\n","                      'best_model': best_model,\n","                      'best_score': best_score,\n","                      'classification_report': best_report # Store the report in results\n","                  }\n","\n","  return results"]},{"cell_type":"markdown","source":["#Run"],"metadata":{"id":"MzUMaQu8S97F"}},{"cell_type":"code","source":["scaling_methods = ['standard', 'minmax', 'quantile', 'l1', 'l2']\n","out = ['no','base','isolation_forest', 'percentile',  'dynamic_threshold']\n","replace = ['no', 'mode', 'mode_all']\n","pca_threshold=0.99\n","dim_reduction = ['LDA', 'PCA', 'no']\n","param_grid = {\n","    'metric': ['euclidean', 'minkowski'],  # Metodologie di distanza\n","    'weights': ['distance', 'uniform'],  # Metodo di ponderazione\n","    'n_neighbors': [2, 3, 5, 10, 15, 20],  # Numero di vicini\n","}"],"metadata":{"id":"DH6HtBM0qN_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Esecuzione dell'esperimento\n","for r in replace:\n","  print(f\"\\n=== Testing Replace Value: {r} ===\")\n","  results = apply(\n","      X_train.copy(), y_train,\n","      X_val, y_val,\n","      scaling_methods, param_grid,\n","      dim_reduction=dim_reduction, pca_threshold=pca_threshold,\n","      target_count=15000, num_datasets=5, random_seed=19,\n","      outs = out,\n","      info = r\n","  )\n","\n","  # Analisi dei risultati\n","  for key, value in results.items():\n","      print(f\"\\n=== Results for {key} ===\")\n","      print(f\"Best Model: {value['best_model']}\")\n","      print(\"Classification Report:\")\n","      print(value['classification_report'])\n","\n","#"],"metadata":{"id":"UnXZWbxKNS-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = joblib.load(\"best_model.pkl\")\n","print(model)"],"metadata":{"id":"fngDNmyjFMIb"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["oK4AXarlSXJf","FJgi_sXUxr9c","EprNSrtG020W","xuYxMmVa9Yyh"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}