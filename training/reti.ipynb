{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SydCnitAeLO_"},"outputs":[],"source":["%%capture\n","import pandas as pd\n","import numpy as np\n","import warnings\n","from google.colab import drive\n","import ipaddress\n","import random\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","\n","warnings.filterwarnings('ignore')\n","drive.mount('/content/drive')\n","\n","FILEPATH = \"/content/drive/MyDrive/data analytics/reti_2.0/results_0.csv\""]},{"cell_type":"markdown","metadata":{"id":"oK4AXarlSXJf"},"source":["# Creazione df result"]},{"cell_type":"code","source":["def create_empty_df(filepath):\n","  \"\"\"\n","  Crea un DataFrame vuoto e lo salva in un file CSV.\n","  \"\"\"\n","  # Definisci le colonne del DataFrame\n","  columns = [\n","      'ds', 'random', 'outlier', 'dim_reduction', 'pca_threshold', 'scaler', 'target count',\n","      'batch_size', 'hidden_size', 'batch_norm', 'dropout', 'depth', 'epoch',\n","      'learning_rate', 'gamma', 'step_size', 'weight_decay', 'info'\n","  ]\n","\n","  # Crea un DataFrame vuoto\n","  results_df = pd.DataFrame(columns=columns)\n","\n","  # Salva il DataFrame in formato CSV\n","  results_df.to_csv(filepath, index=False)\n","\n","  print(f\"DataFrame creato e salvato in {filepath}\")\n","\n","create_empty_df(FILEPATH)"],"metadata":{"id":"_PR2nWm7RUOd","executionInfo":{"status":"ok","timestamp":1738502001008,"user_tz":-60,"elapsed":4,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"db71163a-86f4-4280-c902-f4e186fc3e73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame creato e salvato in /content/drive/MyDrive/data analytics/reti_2.0/results_0.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"FJgi_sXUxr9c"},"source":["# Data Cleaning\n","Scelta colonne, cast delle colonne e gestione dei valori nulli\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05w29vRUj2Q9"},"outputs":[],"source":["# Funzione per determinare il tipo di dato di una colonna\n","def type_data(column):\n","    default_val = [np.nan, '-']\n","    column = column[~column.isin(default_val)]\n","    unique_count = column.nunique()\n","    if is_binary_dtype(column):\n","        return 'Binario'\n","    if  is_numeric_dtype(column):\n","        return 'Numerico Discreto' if pd.api.types.is_integer_dtype(column) else 'Numerico Continuo'\n","    if is_category_dtype(column):\n","        return 'Categorico'\n","    return 'Unknown'\n","\n","# Funzioni ausiliarie per verificare il tipo di dato\n","def is_numeric_dtype(column):\n","    return pd.api.types.is_numeric_dtype(column)\n","\n","def is_binary_dtype(column):\n","    return set(column.unique()) == {True, False}\n","\n","def is_category_dtype(column):\n","    return pd.api.types.is_object_dtype(column) or pd.api.types.is_categorical_dtype(column)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-_JHS8nz5Uv"},"outputs":[],"source":["def clean_service_columns(data):\n","    service_related_cols = {}\n","    categorial_columns = data.select_dtypes(exclude=np.number).columns\n","    categorial_columns = categorial_columns.drop(['dns_qclass', 'dns_qtype', 'http_version', 'http_orig_mime_types', 'http_resp_mime_types'])\n","    for col in categorial_columns:\n","      for prefix in ['dns', 'http', 'ssl']:\n","        if col.startswith(prefix) and not pd.api.types.is_numeric_dtype(col):\n","          if prefix not in service_related_cols:\n","            service_related_cols[prefix] = []\n","          service_related_cols[prefix].append(col)\n","    for col in data.columns:\n","        for service, columns in service_related_cols.items():\n","            if col in columns and f\"service_{service}\" in data.columns:\n","                data.loc[~data[f\"service_{service}\"], col] = '/'\n","    return data\n","\n","def boolean_mapping(value, def_val=None):\n","    if value in {True, False}:\n","        return value\n","    if value == 'T':\n","        return True\n","    if value == 'F':\n","        return False\n","    return def_val if def_val is not None else value\n","\n","def categorize_ports(df, port_columns):\n","    port_bins = [0, 1023, 49151, 65535]\n","    port_labels = [\"Well-Known\", \"Registered\", \"Dynamic\"]\n","    for col in port_columns:\n","        df[col] = pd.cut(df[col], bins=port_bins, labels=port_labels, right=True)\n","    return df\n","\n","\n","def categorize_ip(ip):\n","    try:\n","        ip_obj = ipaddress.ip_address(ip)\n","        if ip_obj.is_loopback:\n","            return \"Loopback\"\n","        if ip_obj.is_private:\n","            return \"Private\"\n","        if ip_obj.is_multicast:\n","            return \"Multicast\"\n","        if ip_obj.is_reserved:\n","            return \"Reserved\"\n","        if ip_obj.is_link_local:\n","            return \"Link-Local\"\n","        return \"Public\"\n","    except ValueError:\n","        return \"Invalid\"\n","\n","def df_mapping(df):\n","  rcode_mapping = {0: 'No Error', 2: 'ServerFailure', 3: 'NameError', 5: 'Refuse'}\n","  qclass_mapping = {0: '-', 1: 'IN', 32769: 'CH'}\n","  qtype_mapping = {0: '-', 1: 'A', 2: 'NS', 5: 'CNAME', 28: 'AAAA', 255: 'ANY'}\n","\n","  for col in df.columns:\n","    if col in ['dns_RD', 'dns_AA', 'dns_rejected', 'http_trans_depth', 'ssl_established','ssl_resumed']:\n","      df[col] = df[col].map(lambda x: boolean_mapping(x)).astype(str)\n","    if col in ['http_status_code', 'weird_addl', 'http_trans_depth']:\n","      df[col] = df[col].astype(str)\n","    if col == 'dns_qclass':\n","      df[col] = df[col].apply(lambda x: qclass_mapping.get(x, None))\n","    if col == 'dns_qtype':\n","      df[col] = df[col].apply(lambda x: qtype_mapping.get(x, None))\n","    if col == 'dns_rcode':\n","      df[col] = df[col].apply(lambda x: rcode_mapping.get(x, None))\n","    if col in ['src_ip', 'dst_ip']:\n","      df[col] = df[col].apply(categorize_ip)\n","    if col == 'src_bytes':\n","      df = df[df['src_bytes'] != '0.0.0.0']\n","      df['src_bytes'] = df['src_bytes'].astype(int)\n","  df = categorize_ports(df, ['src_port', 'dst_port'])\n","  return df\n","\n","def data_cleaning(df):\n","    services = df['service'].str.split(';').explode().unique()  # Estrazione di tutti i servizi unici\n","    for service in services:\n","        df[f'service_{service}'] = df['service'].apply(lambda x: service in x.split(';'))\n","\n","    df.drop(['http_referrer', 'service', 'service_-'], axis=1, inplace=True, errors='ignore')\n","    df.drop(['ts', 'ssl_subject', 'ssl_issuer', 'dns_query', 'http_uri', 'http_user_agent', 'weird_name', 'label'],\n","             axis=1, inplace=True, errors='ignore')\n","\n","    df = df_mapping(df)\n","    df = clean_service_columns(df)\n","\n","    return df"]},{"cell_type":"code","source":["def replace_default_new(df, info):\n","    mode_values = {}\n","    if info == 'mode' or info=='mode_all':\n","      for col in df.columns:\n","        if is_category_dtype(df[col]) or is_binary_dtype(df[col]):\n","            valid_values = df[(df[col] != '/') & (df[col] != '-')][col]\n","            mode_value = valid_values.mode()[0] if not valid_values.empty else '-'  # Usa '-' se non c'Ã¨ moda\n","            mode_values[col] = mode_value\n","\n","            # Sostituzione valori\n","            df[col] = df[col].replace('-', mode_value)\n","            if info == 'mode_all':\n","              df[col] = df[col].replace('/', mode_value)\n","\n","    # Salva le mode con joblib\n","    joblib.dump(mode_values, \"mode.pk\")\n","\n","    return df\n","\n","\n","def apply_saved_modes(val, info):\n","    if info == 'mode' or info=='mode_all':\n","      mode_values = joblib.load(\"mode.pk\")\n","      # Applica le mode ai nuovi dati\n","      for col, mode_value in mode_values.items():\n","          if col in val.columns:\n","              val[col] = val[col].replace('-', mode_value)\n","              if info == 'mode_all':\n","                val[col] = val[col].replace('/', mode_value)\n","    return val"],"metadata":{"id":"qYX3qCVdyIyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7d7wavGkHeZ"},"outputs":[],"source":["df=pd.read_csv('/content/drive/MyDrive/data analytics/train_dataset.csv')\n","df = data_cleaning(df)"]},{"cell_type":"markdown","metadata":{"id":"EprNSrtG020W"},"source":["# Divisione val e train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijcFM87B01a8"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Definisci le features (X) e il target (y)\n","X = df.drop('type', axis=1)  # Assumi che 'label' sia la colonna del target\n","y = df['type']\n","\n","# Dividi il dataset in train e test set\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=19)\n","\n","# Unisci X_train e y_train\n","train_df = pd.concat([X_train, y_train], axis=1)\n","\n","# Unisci X_test e y_test\n","test_df = pd.concat([X_val, y_val], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"xuYxMmVa9Yyh"},"source":["# Pipline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yfSMEUnyn3Q"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, LabelEncoder, Normalizer\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.decomposition import PCA\n","from sklearn.utils import shuffle\n","import joblib\n","from imblearn.over_sampling import SMOTE,  BorderlineSMOTE, RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYW2GAR10g9D"},"outputs":[],"source":["#rimozione outlier per classe\n","def remove_outliers(x, y, out):\n","    x_train = x.copy()\n","    y_train = y.copy()\n","\n","    df = pd.concat([x_train, y_train], axis=1)\n","    numeric_cols = x_train.select_dtypes(include=np.number).columns\n","\n","    # Controlla se non deve essere applicata nessuna rimozione\n","    if out == 'no':\n","        return x, y\n","\n","    if out == 'base':\n","      #rimozione outlier piÃÂ¹ ASSURDI\n","      before = df.shape[0]\n","      df = df[df['duration'] < 1000]\n","      df = df[df['src_bytes']<100000000]\n","      df = df[df['dst_bytes']<100000000]\n","      df = df[df['missed_bytes']<100000000]\n","      df = df[df['src_pkts']<20000]\n","      df = df[df['dst_pkts']<20000]\n","      df = df[df['src_ip_bytes']<1000000]\n","      df = df[df['dst_ip_bytes']<1000000]\n","      print('  Rimosse ',before-df.shape[0],' istanze')\n","      x_train = df.drop('type', axis=1)\n","      y_train = df['type']\n","      return x_train, y_train\n","\n","    filtered_data = []\n","    # Itera su ciascuna classe\n","    for cls in df['type'].unique():\n","        class_df = df[df['type'] == cls]\n","        before = class_df.shape[0]\n","\n","        if out == 'iqr':\n","            for col in numeric_cols:\n","                Q1 = class_df[col].quantile(0.25)\n","                Q3 = class_df[col].quantile(0.75)\n","                IQR = Q3 - Q1\n","                lower_bound = Q1 - 1.5 * IQR\n","                upper_bound = Q3 + 1.5 * IQR\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        elif out == 'percentile':\n","            for col in numeric_cols:\n","                lower_bound = class_df[col].quantile(0.01)\n","                upper_bound = class_df[col].quantile(0.99)\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        elif out == 'isolation_forest':\n","            from sklearn.ensemble import IsolationForest\n","            iso = IsolationForest(contamination=0.05, random_state=19)\n","            numeric_data = class_df[numeric_cols]\n","            class_df['outlier'] = iso.fit_predict(numeric_data)\n","            class_df = class_df[class_df['outlier'] == 1].drop(columns=['outlier'])\n","\n","        elif out == 'dynamic_threshold':\n","            for col in numeric_cols:\n","                mean = class_df[col].mean()\n","                std = class_df[col].std()\n","                lower_bound = mean - 3 * std\n","                upper_bound = mean + 3 * std\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        filtered_data.append(class_df)\n","\n","    # Combina i dati filtrati per ciascuna classe\n","    filtered_df = pd.concat(filtered_data)\n","\n","    x_train = filtered_df.drop('type', axis=1)\n","    y_train = filtered_df['type']\n","\n","    return x_train, y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1jZXJG_ysK9"},"outputs":[],"source":["# scaling and normalization\n","def scale_train_data(x_train, y_train, scaling_method):\n","    scaled_df = x_train.copy()\n","\n","    numeric_columns = x_train.select_dtypes(include=np.number).columns\n","    if len(numeric_columns) == 0:\n","        print(\"  Warning: No numeric columns to scale. Returning original DataFrame.\")\n","        return scaled_df, y_train\n","\n","    if scaling_method == 'none':\n","        print(\"No scaling applied.\")\n","        return scaled_df, y_train\n","    elif scaling_method == 'standard':\n","        scaler = StandardScaler()\n","    elif scaling_method == 'minmax':\n","        scaler = MinMaxScaler()\n","    elif scaling_method == 'quantile':\n","        scaled_df = pd.concat([scaled_df, y_train], axis=1)\n","        scaled_df = scaled_df.sort_values(by='src_bytes')\n","        y_train = scaled_df['type']\n","        scaled_df = scaled_df.drop('type', axis=1)\n","        scaler = QuantileTransformer(output_distribution='uniform', random_state=19)\n","    elif scaling_method == 'l1':\n","        scaler = Normalizer(norm='l1')\n","    elif scaling_method == 'l2':\n","        scaler = Normalizer(norm='l2')\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    if scaled_df[numeric_columns].shape[0] < 1:\n","        print(\"  Warning: Not enough samples to fit the scaler. Returning original DataFrame.\")\n","        return scaled_df, y_train\n","\n","    if scaling_method == 'l1' or scaling_method == 'l2':\n","        scaled_df = scaler.fit_transform(scaled_df)\n","    else:\n","        scaled_df[numeric_columns] = scaler.fit_transform(scaled_df[numeric_columns])\n","    joblib.dump(scaler, \"scaler.pkl\")\n","    return scaled_df, y_train\n","\n","# carica scaler e effettua scaling\n","def scale_validation_data(x_val, y_val, scaling_method):\n","    if scaling_method == 'quantile':\n","        x_val = pd.concat([x_val, y_val], axis=1)\n","        x_val = x_val.sort_values(by='src_bytes')\n","        y_val = x_val['type']\n","        x_val = x_val.drop('type', axis=1)\n","\n","    numeric_columns = x_val.select_dtypes(include=np.number).columns\n","    if scaling_method == 'none':\n","        print(\"No scaling applied to validation data.\")\n","        return x_val, y_val\n","\n","    scaler = joblib.load(\"scaler.pkl\")\n","\n","    if scaling_method == 'l1' or scaling_method == 'l2':\n","        x_val = scaler.transform(x_val)\n","    else:\n","        x_val[numeric_columns] = scaler.transform(x_val[numeric_columns])\n","    return x_val, y_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92oPrwzabh3i"},"outputs":[],"source":["# ENCODING\n","def encode_categorical_train_data(x_train):\n","    categorical_columns = x_train.select_dtypes(include=['object', 'category']).columns\n","\n","    if len(categorical_columns) > 0:\n","        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n","        encoder.fit(x_train[categorical_columns])\n","        joblib.dump(encoder, \"onehot_encoder.pkl\")\n","        x_train_encoded = encoder.transform(x_train[categorical_columns])\n","        encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n","        x_train_encoded_df = pd.DataFrame(x_train_encoded, columns=encoded_feature_names, index=x_train.index)\n","        x_train = x_train.drop(columns=categorical_columns)\n","        x_train = pd.concat([x_train, x_train_encoded_df], axis=1)\n","\n","    return x_train\n","\n","def encode_categorical_validation_data(x_val):\n","    categorical_columns = x_val.select_dtypes(include=['object', 'category']).columns\n","    encoder = joblib.load(\"onehot_encoder.pkl\")\n","\n","    if len(categorical_columns) > 0:\n","        x_val_encoded = encoder.transform(x_val[categorical_columns])\n","        encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n","        x_val_encoded_df = pd.DataFrame(x_val_encoded, columns=encoded_feature_names, index=x_val.index)\n","        x_val = x_val.drop(columns=categorical_columns)\n","        x_val = pd.concat([x_val, x_val_encoded_df], axis=1)\n","\n","    return x_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbCAdHq90VFz"},"outputs":[],"source":["# BILANCIAMENTO\n","def balance_data(x_train, y_train, target_count, num_datasets, random_seed):\n","    smote = SMOTE(random_state=random_seed)\n","    oversampler = RandomOverSampler(random_state=random_seed)\n","\n","    class_counts = pd.Series(y_train).value_counts()\n","    smote_classes = [cls for cls in class_counts.index if class_counts[cls] < target_count / 2]\n","\n","    if smote_classes:\n","        smote_strategy = {cls: target_count for cls in smote_classes}\n","        smote = SMOTE(sampling_strategy=smote_strategy, random_state=random_seed)\n","        x_train, y_train = smote.fit_resample(x_train, y_train)\n","        class_counts = pd.Series(y_train).value_counts()\n","\n","    over_classes = [cls for cls in class_counts.index if class_counts[cls] < target_count]\n","    if over_classes:\n","        over_strategy = {cls: target_count for cls in over_classes}\n","        oversampler = RandomOverSampler(sampling_strategy=over_strategy, random_state=random_seed)\n","        x_train, y_train = oversampler.fit_resample(x_train, y_train)\n","\n","    datasets = []\n","    for i in range(num_datasets):\n","        undersampler = RandomUnderSampler(sampling_strategy={cls: target_count for cls in pd.Series(y_train).value_counts().index}, random_state=random_seed + i)\n","        x_resampled, y_resampled = undersampler.fit_resample(x_train, y_train)\n","        x_resampled, y_resampled = shuffle(x_resampled, y_resampled, random_state=random_seed + i)\n","        datasets.append((x_resampled, y_resampled))\n","\n","\n","\n","    return datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Atst5e5-0c_H"},"outputs":[],"source":["# PCA\n","def apply_pca_train(x_train, random_state, pca_threshold=0.99):\n","    pca = PCA(random_state=random_state)\n","    pca.fit(x_train)\n","    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n","    n_components = (cumulative_variance >= pca_threshold).argmax() + 1\n","    pca = PCA(n_components=n_components, random_state=random_state)\n","    transformed_data = pca.fit_transform(x_train)\n","    transformed_data = transformed_data.astype(np.float32)\n","\n","    print(f\"  Numero di colonne selezionate (componenti principali): {n_components}\")\n","    joblib.dump(pca, \"pca_model.pkl\")\n","    return pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n","\n","def apply_pca_validation(x_val):\n","    pca = joblib.load(\"pca_model.pkl\")\n","    x_val = pca.transform(x_val)\n","    x_val = x_val.astype(np.float32)\n","    return x_val\n","\n","# LDA\n","def apply_lda_train(x_train, y_train, lda_components=None):\n","    lda = LDA(n_components=lda_components)\n","    lda.fit(x_train, y_train)\n","    transformed_data = lda.transform(x_train)\n","    transformed_data = transformed_data.astype(np.float32)\n","\n","    n_components = transformed_data.shape[1]\n","    print(f\"  Numero di colonne selezionate (componenti discriminanti): {n_components}\")\n","    joblib.dump(lda, \"lda_model.pkl\")\n","    return pd.DataFrame(transformed_data, columns=[f\"LD{i+1}\" for i in range(n_components)])\n","\n","def apply_lda_validation(x_val):\n","    lda = joblib.load(\"lda_model.pkl\")\n","    x_val = lda.transform(x_val)\n","    x_val = x_val.astype(np.float32)\n","    return x_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YueC9OPp9afg"},"outputs":[],"source":["def preprocessing_pipeline(x_train, y_train, x_validation, y_validation, scaling_method, use_pca, pca_threshold, target_count=20000, num_datasets=1, random_seed=19):\n","    # Encoding delle feature\n","    x_train = encode_categorical_train_data(x_train)\n","    x_validation = encode_categorical_validation_data(x_validation)\n","\n","    # Bilanciamento\n","    datasets = balance_data(x_train, y_train, target_count, num_datasets, random_seed)\n","    validation = []\n","    data = []\n","    i = 0\n","\n","    for x_train, y_train in datasets:\n","      print(f\"  Dataset bilanciato {i+1}:\")\n","      i+=1\n","      # Scaling\n","      x_train, y_train = scale_train_data(x_train, y_train, scaling_method)\n","      x_val, y_val = scale_validation_data(x_validation, y_validation, scaling_method)\n","\n","      if use_pca == 'PCA':\n","          x_train = apply_pca_train(x_train, random_state=random_seed, pca_threshold=pca_threshold)\n","          x_val = apply_pca_validation(x_val)\n","      elif use_pca == 'LDA':\n","          x_train = apply_lda_train(x_train, y_train)\n","          x_val = apply_lda_validation(x_val)\n","\n","      # Bilanciamento\n","      data.append((x_train, y_train))\n","      validation.append((x_val, y_val))\n","    return data, validation\n"]},{"cell_type":"markdown","metadata":{"id":"lTUGMeKlHm1S"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jez8IsOIKtHp"},"outputs":[],"source":["def fix_random(seed: int) -> None:\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True  # slower\n","\n","# Verifica se una combinazione Ã¨ giÃ  presente nel file CSV.\n","def is_combination_tested(filepath, new_row, num_epochs):\n","  return False\n","  existing_results = pd.read_csv(filepath)\n","  # Colonne per la comparazione\n","  comparison_columns = [\n","      'ds', 'random', 'outlier', 'dim_reduction', 'pca_threshold', 'scaler', 'target count',\n","      'batch_size', 'hidden_size', 'batch_norm', 'dropout', 'depth',\n","      'learning_rate', 'gamma', 'step_size', 'weight_decay'\n","  ]\n","\n","  # Aggiungi colonne mancanti a `new_row` con valore NaN\n","  for col in comparison_columns:\n","    if col not in new_row:\n","        new_row[col] = np.nan\n","\n","  # Filtra righe che corrispondono ai valori di new_row\n","  filtered_results = existing_results.copy()\n","  filtered_results = filtered_results[filtered_results['epoch'] == num_epochs]\n","  for col in new_row.keys():\n","    # Mantieni solo le righe in cui i valori corrispondono (o sono entrambi NaN)\n","    filtered_results = filtered_results[\n","        (filtered_results[col] == new_row[col]) | (pd.isna(filtered_results[col]) & pd.isna(new_row[col]))\n","    ]\n","\n","  # Controlla se tutte le colonne non in new_row sono NaN\n","  for _, row in filtered_results.iterrows():\n","    all_remaining_nan = all(pd.isna(row[col]) for col in comparison_columns if col not in new_row)\n","    if all_remaining_nan:\n","        print(\"  Configurazione giÃ  testata, salto...\")\n","        return True\n","  return False\n","\n","def append_and_save_results(filepath, new_row):\n","  results_df = pd.read_csv(filepath)\n","  results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n","  results_df.to_csv(filepath, index=False)"]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self, X, y):\n","        if isinstance(X, (pd.DataFrame, pd.Series)):\n","            X = X.values\n","        if isinstance(y, (pd.DataFrame, pd.Series)):\n","            y = y.values\n","        self.X = torch.FloatTensor(X)\n","        self.y = torch.LongTensor(y)\n","\n","        self.num_features = X.shape[1]\n","        self.num_classes = len(np.unique(y))\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx, :], self.y[idx]\n","\n","class FeedForwardPlus(nn.Module):\n","    def __init__(self, input_size, num_classes, hidden_size, depth=1, batch_norm=False, drop=0):\n","        super(FeedForwardPlus, self).__init__()\n","\n","        model = []\n","        model += [nn.Linear(input_size, hidden_size)]\n","        if batch_norm:\n","            model += [nn.BatchNorm1d(hidden_size)]\n","        model += [nn.ReLU()]\n","\n","        block = [nn.Linear(hidden_size, hidden_size), nn.ReLU()]\n","        block_batch_norm = [nn.Linear(hidden_size, hidden_size), nn.BatchNorm1d(hidden_size), nn.ReLU()]\n","        block_dropout = [nn.Dropout(drop), nn.Linear(hidden_size, hidden_size), nn.ReLU()]\n","\n","        for i in range(depth):\n","            if not batch_norm and drop == 0:\n","                model += block\n","            elif batch_norm and drop == 0:\n","                model += block_batch_norm\n","            elif drop > 0 and not batch_norm:\n","                model += block_dropout\n","\n","        self.model = nn.Sequential(*model)\n","        self.output = nn.Linear(hidden_size, num_classes)\n","\n","    def forward(self, x):\n","        h = self.model(x)\n","        out = self.output(h)\n","        return out"],"metadata":{"id":"Tq9-L4XRcuWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_model(model, data_loader, device):\n","    model = model.to(device)\n","    model.eval()  # Imposta il modello in modalitÃ  di valutazione\n","\n","    y_pred = []\n","    y_test = []\n","\n","    with torch.no_grad():\n","      for data, targets in data_loader:\n","          # Sposta i dati e i target sulla GPU (o CPU se non disponibile)\n","          data, targets = data.to(device), targets.to(device)\n","\n","          # Calcola le predizioni\n","          output = model(data)\n","          y_pred.append(output)\n","          y_test.append(targets)\n","\n","    # Unisci tutte le predizioni e i target in un unico tensor\n","    y_test = torch.cat(y_test).squeeze()\n","    y_pred = torch.cat(y_pred).squeeze()\n","    # Determina le classi previste (indice della classe con probabilitÃ  massima)\n","    y_pred_c = y_pred.argmax(dim=1, keepdim=True).squeeze()\n","    return y_test, y_pred_c, y_pred\n","\n","\n","def train_model(model, criterion, optimizer, epoch, scheduler, train_loader, val_loader, device, new_row):\n","    n_iter = 0\n","\n","    best_valid_loss = float('inf')\n","    best_model = None\n","    best_accuracy = 0\n","    best_report = None\n","\n","    # per ogni epoca\n","    for epoch in range(epoch):\n","        new_row['epoch'] = epoch+1\n","        model.train()\n","        # per ogni batch\n","        for data, targets in train_loader:\n","            data, targets = data.to(device), targets.to(device)\n","            optimizer.zero_grad() # resetta i gradienti\n","            y_pred = model(data) # Forward pass\n","            loss = criterion(y_pred, targets) # Compute Loss\n","            loss.backward() # Backward pass\n","            optimizer.step()\n","\n","            n_iter += 1\n","\n","        # valutazione sul validation\n","        labels, y_pred_c, y_pred = test_model(model, val_loader, device)\n","        loss_val = criterion(y_pred, labels)\n","        val_accuracy = accuracy_score(labels, y_pred_c)\n","        report = classification_report(labels, y_pred_c, output_dict=True)\n","        print(report)\n","\n","        # train sul validation\n","        labels_t, y_pred_c_t, y_pred_t = test_model(model, train_loader, device)\n","        loss_t = criterion(y_pred_t, labels_t)\n","        t_accuracy = accuracy_score(labels_t, y_pred_c_t)\n","        t_report = classification_report(labels_t, y_pred_c_t, output_dict=True)\n","\n","        # Valutazione\n","        new_row.update({\n","            'val_accuracy': report['accuracy'],\n","            'val_precision': report['weighted avg']['precision'],\n","            'val_recall': report['weighted avg']['recall'],\n","            'val_f1': report['weighted avg']['f1-score'],\n","            'validation_loss': loss_val.item(),\n","            'train_accuracy': t_report['accuracy'],\n","            'train_precision': t_report['weighted avg']['precision'],\n","            'train_recall': t_report['weighted avg']['recall'],\n","            'train_f1': t_report['weighted avg']['f1-score'],\n","            'train_loss': loss_t.item()\n","        })\n","\n","        # Salva i risultati\n","        append_and_save_results(FILEPATH, new_row)\n","        if report['accuracy']> best_accuracy:\n","          best_accuracy = report['accuracy']\n","          best_model = model\n","          best_report = report\n","          best_valid_loss = loss_val.item()\n","        scheduler.step()\n","        print(f\"{new_row['epoch']}. Validation score: {val_accuracy} - Train score {t_accuracy} -  Validation loss: {loss_val.item()} - Train Loss: {loss_t.item()} \")\n","\n","    return best_model, best_accuracy, {'validation_score': val_accuracy, 'train_score': t_accuracy, 'validation_loss': {loss_val.item()}, 'train_loss': {loss_t.item()}, 'classification_report': report}"],"metadata":{"id":"7CXhd7jHc9mn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7O71MA0xtwq_"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","from itertools import product\n","import time\n","from sklearn.preprocessing import LabelEncoder\n","\n","def dnn_with_grid(x_train, y_train, x_val, y_val, param_grid, metadata, random_state = 19, scoring='accuracy'):\n","  keys, values = zip(*param_grid.items())\n","  param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n","\n","  best_score = -float('inf')\n","  best_model = None\n","  best_report = None\n","\n","  le = LabelEncoder()\n","  y_train = le.fit_transform(y_train)  # Converte le categorie in interi\n","  y_val = le.transform(y_val)          # Trasforma anche il validation set\n","  joblib.dump(le, \"label_encoder.pkl\")\n","\n","  # Create the dataset\n","  train_dataset = MyDataset(x_train,y_train)\n","  val_dataset = MyDataset(x_val,y_val)\n","\n","  for params in param_combinations:\n","      fix_random(random_state)\n","      print(f\"Valutando configurazione: {params}\")\n","\n","      # Estrai i parametri dalla configurazione attuale\n","      batch_size = params['batch_size']\n","      hidden_size = params['hidden_size']\n","      depth = params['depth']\n","      batch_norm = params['batch_norm']\n","      learning_rate = params['learning_rate']\n","      step_size = params['step_size']\n","      gamma = params['gamma']\n","      num_epochs = params['num_epochs']\n","      weight_decay = params['weight_decay']\n","\n","      # Crea una nuova riga di metadata\n","      new_row = metadata.copy()\n","      new_row.update(params)\n","\n","\n","      # Verifica se la configurazione Ã¨ giÃ  testata\n","      if not is_combination_tested(FILEPATH, new_row, num_epochs):\n","            start = time.time()\n","\n","            # Create relative dataloaders\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","            val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","            #define architecture, loss and optimizer\n","            model = FeedForwardPlus(train_dataset.num_features, train_dataset.num_classes, hidden_size, depth, batch_norm=batch_norm)\n","            model.to(device)\n","\n","            #train\n","            criterion = torch.nn.CrossEntropyLoss()\n","            #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n","            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","            model, accuracy, results = train_model(model, criterion, optimizer, num_epochs, scheduler, train_loader, val_loader, device, new_row)\n","\n","            print(\"time elapsed:\", time.time() - start)\n","\n","            if accuracy > best_score:\n","                print('interessante')\n","                best_score = accuracy\n","                best_model = model\n","                balance_data = results['classification_report']\n","\n","  return best_model, best_score, best_report"]},{"cell_type":"code","source":["def apply(x_train, y_train, x_val, y_val, scaling_methods, param_grid,\n","  dim_reduction=['no'], pca_threshold=0.99, target_count=20000, num_datasets=1,\n","  random_seed=19, outs=['no'], info=''):\n","  \"\"\"\n","  Esegue l'intera pipeline di preprocessing, training e valutazione.\n","  \"\"\"\n","  results = {}\n","  x_train_fix = x_train\n","  x_val_fix = x_val\n","  bestbest = 0\n","\n","  for dim_redx in dim_reduction:\n","    print(f\"\\n=== Testing Dimensionality Reduction: {dim_redx} ===\")\n","    if dim_redx != 'PCA':\n","      pca_threshold = None\n","    for scaling_method in scaling_methods:\n","      for out in outs:\n","          print(f\"\\n=== Testing Scaling Method: {scaling_method}, Outlier: {out} ===\")\n","\n","          x_train = replace_default_new(x_train_fix.copy(), info)\n","          x_val = apply_saved_modes(x_val_fix.copy(), info)\n","\n","          # Rimuovi outlier\n","          x_train_filtered, y_train_filtered = remove_outliers(x_train, y_train, out)\n","\n","          # Preprocessing\n","          datasets, validation = preprocessing_pipeline(x_train_filtered, y_train_filtered, x_val, y_val,\n","              scaling_method, dim_redx, pca_threshold, target_count, num_datasets, random_seed\n","          )\n","\n","          results = {}\n","          for i, (x_train_processed, y_train_processed) in enumerate(datasets):\n","              x_val_processed, y_val_processed = validation[i]\n","              ds_name = f\"dataset_{i+1}\"\n","              metadata = {\n","                  'ds': ds_name,\n","                  'random': random_seed,\n","                  'outlier': out,\n","                  'dim_reduction': dim_redx,\n","                  'pca_threshold': pca_threshold,\n","                  'scaler': scaling_method,\n","                  'target count': target_count,\n","                  'info': info\n","              }\n","\n","              print(f\"--- Training Dataset {i+1}/{len(datasets)} ---\")\n","              best_model, best_score, best_report = dnn_with_grid(\n","                  x_train_processed, y_train_processed,\n","                  x_val_processed, y_val_processed,\n","                  param_grid, metadata, random_seed\n","              )\n","              if best_score > bestbest:\n","                bestbest = best_score\n","                joblib.dump(best_model, \"best_model.pkl\")\n","                print(best_score)\n","                print(best_report)\n","              if best_model:\n","                  results[f\"{scaling_method}_dataset_{i+1}\"] = {\n","                      'best_model': best_model,\n","                      'best_score': best_score,\n","                      'classification_report': best_report # Store the report in results\n","                  }\n","\n","              print(f\"--- Training Dataset {i+1}/{len(datasets)} ---\")\n","              best_model, best_score, best_report = dnn_with_grid(\n","                  x_train_processed, y_train_processed,\n","                  x_val_processed, y_val_processed,\n","                  param_grid, metadata, random_seed\n","              )\n","              if best_model:\n","                  results[f\"{scaling_method}_dataset_{i+1}\"] = {\n","                      'best_model': best_model,\n","                      'best_score': best_score,\n","                      'classification_report': best_report # Store the report in results\n","                  }\n","                  joblib.dump(best_model, f\"best_model.pkl\")\n","                  print(best_score)\n","                  print(best_report)\n","  return results"],"metadata":{"id":"DNcq_g2mezns"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Run"],"metadata":{"id":"MzUMaQu8S97F"}},{"cell_type":"code","source":["scaling_methods = ['l1', 'l2','minmax',]\n","out = ['no','base','isolation_forest', 'percentile',  'dynamic_threshold']\n","replace = ['no', 'mode', 'mode_all']\n","pca_threshold=0.99\n","dim_reduction = ['LDA', 'PCA', 'no']\n","param_grid = {\n","    \"batch_size\": [16, 32, 64, 128],\n","    \"hidden_size\": [16, 32, 64, 128, 192, 256, 512],\n","    \"batch_norm\": [False, True],\n","    \"dropout\": [0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n","    \"depth\": [2, 3, 4, 5, 6, 7, 8, 9, 10],\n","    \"weight_decay\": [1e-06, 1e-05, 0.001, 0.1],\n","    \"learning_rate\": [1.0e-05, 1.0e-04, 1.0e-03, 1.0e-02, 2.0e-03, 2.0e-05,\n","                      5.0e-04, 5.0e-05, 5.0e-03, 9.0e-04, 1.2e-03, 1.0e-01],\n","    \"gamma\": [0.1, 0.3, 0.4, 0.5, 0.8, 0.85, 0.9, 1.0],\n","    \"step_size\": [7.5, 10, 12, 12.5, 15, 20, 22, 25, 50],\n","    \"num_epochs\": [80],\n","}"],"metadata":{"id":"DH6HtBM0qN_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# look for GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('mps')\n","print(\"Device: {}\".format(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAJMD904wnLv","executionInfo":{"status":"ok","timestamp":1738501803237,"user_tz":-60,"elapsed":4,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}},"outputId":"68d3e2c9-9a31-47ff-b756-6850de6dab66"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cpu\n"]}]},{"cell_type":"code","source":["# Esecuzione dell'esperimento\n","for r in replace:\n","  print(f\"\\n=== Testing Replace Value: {r} ===\")\n","  results = apply(\n","      X_train.copy(), y_train,\n","      X_val, y_val,\n","      scaling_methods, param_grid,\n","      dim_reduction=dim_reduction, pca_threshold=pca_threshold,\n","      target_count=20000, num_datasets=5, random_seed=19,\n","      outs = out,\n","      info = r\n","  )\n","\n","  # Analisi dei risultati\n","  for key, value in results.items():\n","      print(f\"\\n=== Results for {key} ===\")\n","      print(f\"Best Model: {value['best_model']}\")\n","      print(\"Classification Report:\")\n","      print(value['classification_report'])"],"metadata":{"id":"UnXZWbxKNS-0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = joblib.load(\"best_model_temp.pkl\")\n","print(model)"],"metadata":{"id":"fngDNmyjFMIb"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["oK4AXarlSXJf","EprNSrtG020W","xuYxMmVa9Yyh"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}