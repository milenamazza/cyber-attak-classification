{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"SydCnitAeLO_"},"outputs":[],"source":["%%capture\n","import pandas as pd\n","import numpy as np\n","import warnings\n","from google.colab import drive\n","import ipaddress\n","import random\n","import os\n","\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch.utils.tensorboard import SummaryWriter\n","\n","\n","warnings.filterwarnings('ignore')\n","drive.mount('/content/drive')\n","\n","FILEPATH = \"/content/drive/MyDrive/data analytics/tabtrasf/results_0.csv\""]},{"cell_type":"markdown","metadata":{"id":"oK4AXarlSXJf"},"source":["# Creazione df result"]},{"cell_type":"code","source":["def create_empty_df(filepath):\n","  \"\"\"\n","  Crea un DataFrame vuoto e lo salva in un file CSV.\n","  \"\"\"\n","  # Definisci le colonne del DataFrame\n","  columns = [\n","      'ds', 'random', 'outlier', 'dim_reduction', 'pca_threshold', 'scaler', 'target count',\n","      'batch_size', 'dim_embedding', 'num_heads', 'num_layers', 'learning_rate', 'epoch',\n","      'gamma', 'step_size', 'weight_decay', 'info'\n","  ]\n","  # Crea un DataFrame vuoto\n","  results_df = pd.DataFrame(columns=columns)\n","\n","  # Salva il DataFrame in formato CSV\n","  results_df.to_csv(filepath, index=False)\n","\n","  print(f\"DataFrame creato e salvato in {filepath}\")\n","\n","create_empty_df(FILEPATH)"],"metadata":{"id":"_PR2nWm7RUOd","executionInfo":{"status":"ok","timestamp":1738508495127,"user_tz":-60,"elapsed":10,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d22d9e90-34d5-42e1-8868-53271b20a145"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame creato e salvato in /content/drive/MyDrive/data analytics/tabtrasf/results_0.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"FJgi_sXUxr9c"},"source":["# Data Cleaning\n","Scelta colonne, cast delle colonne e gestione dei valori nulli\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"05w29vRUj2Q9"},"outputs":[],"source":["# Funzione per determinare il tipo di dato di una colonna\n","def type_data(column):\n","    default_val = [np.nan, '-']\n","    column = column[~column.isin(default_val)]\n","    unique_count = column.nunique()\n","    if is_binary_dtype(column):\n","        return 'Binario'\n","    if  is_numeric_dtype(column):\n","        return 'Numerico Discreto' if pd.api.types.is_integer_dtype(column) else 'Numerico Continuo'\n","    if is_category_dtype(column):\n","        return 'Categorico'\n","    return 'Unknown'\n","\n","# Funzioni ausiliarie per verificare il tipo di dato\n","def is_numeric_dtype(column):\n","    return pd.api.types.is_numeric_dtype(column)\n","\n","def is_binary_dtype(column):\n","    return set(column.unique()) == {True, False}\n","\n","def is_category_dtype(column):\n","    return pd.api.types.is_object_dtype(column) or pd.api.types.is_categorical_dtype(column)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-_JHS8nz5Uv"},"outputs":[],"source":["def clean_service_columns(data):\n","    service_related_cols = {}\n","    categorial_columns = data.select_dtypes(exclude=np.number).columns\n","    for col in categorial_columns:\n","      for prefix in ['dns', 'http', 'ssl']:\n","        if col.startswith(prefix) and not pd.api.types.is_numeric_dtype(col):\n","          if prefix not in service_related_cols:\n","            service_related_cols[prefix] = []\n","          service_related_cols[prefix].append(col)\n","    for col in data.columns:\n","        for service, columns in service_related_cols.items():\n","            if col in columns and f\"service_{service}\" in data.columns:\n","                data.loc[~data[f\"service_{service}\"], col] = '/'\n","    return data\n","\n","def boolean_mapping(value, def_val=None):\n","    if value in {True, False}:\n","        return value\n","    if value == 'T':\n","        return True\n","    if value == 'F':\n","        return False\n","    return def_val if def_val is not None else value\n","\n","def categorize_ports(df, port_columns):\n","    port_bins = [0, 1023, 49151, 65535]\n","    port_labels = [\"Well-Known\", \"Registered\", \"Dynamic\"]\n","    for col in port_columns:\n","        df[col] = pd.cut(df[col], bins=port_bins, labels=port_labels, right=True)\n","    return df\n","\n","\n","def categorize_ip(ip):\n","    try:\n","        ip_obj = ipaddress.ip_address(ip)\n","        if ip_obj.is_loopback:\n","            return \"Loopback\"\n","        if ip_obj.is_private:\n","            return \"Private\"\n","        if ip_obj.is_multicast:\n","            return \"Multicast\"\n","        if ip_obj.is_reserved:\n","            return \"Reserved\"\n","        if ip_obj.is_link_local:\n","            return \"Link-Local\"\n","        return \"Public\"\n","    except ValueError:\n","        return \"Invalid\"\n","\n","def df_mapping(df):\n","  rcode_mapping = {0: 'No Error', 2: 'ServerFailure', 3: 'NameError', 5: 'Refuse'}\n","  qclass_mapping = {0: '-', 1: 'IN', 32769: 'CH'}\n","  qtype_mapping = {0: '-', 1: 'A', 2: 'NS', 5: 'CNAME', 28: 'AAAA', 255: 'ANY'}\n","\n","  for col in df.columns:\n","    if col in ['dns_RD', 'dns_RA', 'dns_AA', 'dns_rejected', 'ssl_established', 'ssl_resumed', 'weird_notice']:\n","      df[col] = df[col].map(lambda x: boolean_mapping(x)).astype(str)\n","    if col in ['http_status_code', 'weird_addl', 'http_trans_depth']:\n","      df[col] = df[col].astype(str)\n","    if col == 'dns_qclass':\n","      df[col] = df[col].apply(lambda x: qclass_mapping.get(x, None))\n","    if col == 'dns_qtype':\n","      df[col] = df[col].apply(lambda x: qtype_mapping.get(x, None))\n","    if col == 'dns_rcode':\n","      df[col] = df[col].apply(lambda x: rcode_mapping.get(x, None))\n","    if col in ['src_ip', 'dst_ip']:\n","      df[col] = df[col].apply(categorize_ip)\n","    if col == 'src_bytes':\n","      df = df[df['src_bytes'] != '0.0.0.0']\n","      df['src_bytes'] = df['src_bytes'].astype(int)\n","  df = categorize_ports(df, ['src_port', 'dst_port'])\n","  return df\n","\n","def data_cleaning(df):\n","    services = df['service'].str.split(';').explode().unique()  # Estrazione di tutti i servizi unici\n","    for service in services:\n","        df[f'service_{service}'] = df['service'].apply(lambda x: service in x.split(';'))\n","\n","    df.drop(['http_referrer', 'service', 'service_-'], axis=1, inplace=True, errors='ignore')\n","    df.drop(['ts', 'ssl_subject', 'ssl_issuer', 'dns_query', 'http_uri', 'http_user_agent', 'weird_name', 'label'],\n","             axis=1, inplace=True, errors='ignore')\n","\n","    df = df_mapping(df)\n","    df = clean_service_columns(df)\n","\n","    return df"]},{"cell_type":"code","source":["def replace_default_new(df, info):\n","    mode_values = {}\n","    if info == 'mode' or info=='mode_all':\n","      for col in df.columns:\n","        if is_category_dtype(df[col]) or is_binary_dtype(df[col]):\n","            valid_values = df[(df[col] != '/') & (df[col] != '-')][col]\n","            mode_value = valid_values.mode()[0] if not valid_values.empty else '-'  # Usa '-' se non c'Ã¨ moda\n","            mode_values[col] = mode_value\n","\n","            # Sostituzione valori\n","            df[col] = df[col].replace('-', mode_value)\n","            if info == 'mode_all':\n","              df[col] = df[col].replace('/', mode_value)\n","\n","    # Salva le mode con joblib\n","    joblib.dump(mode_values, \"mode.pk\")\n","\n","    return df\n","\n","\n","def apply_saved_modes(val, info):\n","    if info == 'mode' or info=='mode_all':\n","      mode_values = joblib.load(\"mode.pk\")\n","      # Applica le mode ai nuovi dati\n","      for col, mode_value in mode_values.items():\n","          if col in val.columns:\n","              val[col] = val[col].replace('-', mode_value)\n","              if info == 'mode_all':\n","                val[col] = val[col].replace('/', mode_value)\n","    return val"],"metadata":{"id":"qYX3qCVdyIyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d7d7wavGkHeZ"},"outputs":[],"source":["df=pd.read_csv('/content/drive/MyDrive/data analytics/train_dataset.csv')\n","df = data_cleaning(df)"]},{"cell_type":"markdown","metadata":{"id":"EprNSrtG020W"},"source":["# Divisione val e train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ijcFM87B01a8"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Definisci le features (X) e il target (y)\n","X = df.drop('type', axis=1)  # Assumi che 'label' sia la colonna del target\n","y = df['type']\n","\n","# Dividi il dataset in train e test set\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=19)\n","\n","# Unisci X_train e y_train\n","train_df = pd.concat([X_train, y_train], axis=1)\n","\n","# Unisci X_test e y_test\n","test_df = pd.concat([X_val, y_val], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"xuYxMmVa9Yyh"},"source":["# Pipline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yfSMEUnyn3Q"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, LabelEncoder, Normalizer\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.decomposition import PCA\n","from sklearn.utils import shuffle\n","import joblib\n","from imblearn.over_sampling import SMOTE,  BorderlineSMOTE, RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYW2GAR10g9D"},"outputs":[],"source":["#rimozione outlier per classe\n","def remove_outliers(x, y, out):\n","    x_train = x.copy()\n","    y_train = y.copy()\n","\n","    df = pd.concat([x_train, y_train], axis=1)\n","    numeric_cols = x_train.select_dtypes(include=np.number).columns\n","\n","    # Controlla se non deve essere applicata nessuna rimozione\n","    if out == 'no':\n","        return x, y\n","\n","    if out == 'base':\n","      #rimozione outlier piÃÂ¹ ASSURDI\n","      before = df.shape[0]\n","      df = df[df['duration'] < 1000]\n","      df = df[df['src_bytes']<100000000]\n","      df = df[df['dst_bytes']<100000000]\n","      df = df[df['missed_bytes']<100000000]\n","      df = df[df['src_pkts']<20000]\n","      df = df[df['dst_pkts']<20000]\n","      df = df[df['src_ip_bytes']<1000000]\n","      df = df[df['dst_ip_bytes']<1000000]\n","      print('  Rimosse ',before-df.shape[0],' istanze')\n","      x_train = df.drop('type', axis=1)\n","      y_train = df['type']\n","      return x_train, y_train\n","\n","    filtered_data = []\n","    # Itera su ciascuna classe\n","    for cls in df['type'].unique():\n","        class_df = df[df['type'] == cls]\n","        before = class_df.shape[0]\n","\n","        if out == 'iqr':\n","            for col in numeric_cols:\n","                Q1 = class_df[col].quantile(0.25)\n","                Q3 = class_df[col].quantile(0.75)\n","                IQR = Q3 - Q1\n","                lower_bound = Q1 - 1.5 * IQR\n","                upper_bound = Q3 + 1.5 * IQR\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        elif out == 'percentile':\n","            for col in numeric_cols:\n","                lower_bound = class_df[col].quantile(0.01)\n","                upper_bound = class_df[col].quantile(0.99)\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        elif out == 'isolation_forest':\n","            from sklearn.ensemble import IsolationForest\n","            iso = IsolationForest(contamination=0.05, random_state=19)\n","            numeric_data = class_df[numeric_cols]\n","            class_df['outlier'] = iso.fit_predict(numeric_data)\n","            class_df = class_df[class_df['outlier'] == 1].drop(columns=['outlier'])\n","\n","        elif out == 'dynamic_threshold':\n","            for col in numeric_cols:\n","                mean = class_df[col].mean()\n","                std = class_df[col].std()\n","                lower_bound = mean - 3 * std\n","                upper_bound = mean + 3 * std\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        filtered_data.append(class_df)\n","\n","    # Combina i dati filtrati per ciascuna classe\n","    filtered_df = pd.concat(filtered_data)\n","\n","    x_train = filtered_df.drop('type', axis=1)\n","    y_train = filtered_df['type']\n","\n","    return x_train, y_train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G1jZXJG_ysK9"},"outputs":[],"source":["# scaling and normalization\n","def scale_train_data(x_train, y_train, scaling_method):\n","    scaled_df = x_train.copy()\n","\n","    numeric_columns = x_train.select_dtypes(include=np.number).columns\n","    if len(numeric_columns) == 0:\n","        print(\"  Warning: No numeric columns to scale. Returning original DataFrame.\")\n","        return scaled_df, y_train\n","\n","    if scaling_method == 'none':\n","        print(\"No scaling applied.\")\n","        return scaled_df, y_train\n","    elif scaling_method == 'standard':\n","        scaler = StandardScaler()\n","    elif scaling_method == 'minmax':\n","        scaler = MinMaxScaler()\n","    elif scaling_method == 'quantile':\n","        scaled_df = pd.concat([scaled_df, y_train], axis=1)\n","        scaled_df = scaled_df.sort_values(by='src_bytes')\n","        y_train = scaled_df['type']\n","        scaled_df = scaled_df.drop('type', axis=1)\n","        scaler = QuantileTransformer(output_distribution='uniform', random_state=19)\n","    elif scaling_method == 'l1':\n","        scaler = Normalizer(norm='l1')\n","    elif scaling_method == 'l2':\n","        scaler = Normalizer(norm='l2')\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    if scaled_df[numeric_columns].shape[0] < 1:\n","        print(\"  Warning: Not enough samples to fit the scaler. Returning original DataFrame.\")\n","        return scaled_df, y_train\n","\n","    if scaling_method == 'l1' or scaling_method == 'l2':\n","        scaled_df = scaler.fit_transform(scaled_df)\n","    else:\n","        scaled_df[numeric_columns] = scaler.fit_transform(scaled_df[numeric_columns])\n","    joblib.dump(scaler, \"scaler.pkl\")\n","    return scaled_df, y_train\n","\n","# carica scaler e effettua scaling\n","def scale_validation_data(x_val, y_val, scaling_method):\n","    if scaling_method == 'quantile':\n","        x_val = pd.concat([x_val, y_val], axis=1)\n","        x_val = x_val.sort_values(by='src_bytes')\n","        y_val = x_val['type']\n","        x_val = x_val.drop('type', axis=1)\n","\n","    numeric_columns = x_val.select_dtypes(include=np.number).columns\n","    if scaling_method == 'none':\n","        print(\"No scaling applied to validation data.\")\n","        return x_val, y_val\n","\n","    scaler = joblib.load(\"scaler.pkl\")\n","\n","    if scaling_method == 'l1' or scaling_method == 'l2':\n","        x_val = scaler.transform(x_val)\n","    else:\n","        x_val[numeric_columns] = scaler.transform(x_val[numeric_columns])\n","    return x_val, y_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"92oPrwzabh3i"},"outputs":[],"source":["# ENCODING\n","def encode_categorical_train_data(x_train):\n","    categorical_columns = x_train.select_dtypes(include=['object', 'category']).columns\n","\n","    if len(categorical_columns) > 0:\n","        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n","        encoder.fit(x_train[categorical_columns])\n","        joblib.dump(encoder, \"onehot_encoder.pkl\")\n","        x_train_encoded = encoder.transform(x_train[categorical_columns])\n","        encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n","        x_train_encoded_df = pd.DataFrame(x_train_encoded, columns=encoded_feature_names, index=x_train.index)\n","        x_train = x_train.drop(columns=categorical_columns)\n","        x_train = pd.concat([x_train, x_train_encoded_df], axis=1)\n","\n","    return x_train\n","\n","def encode_categorical_validation_data(x_val):\n","    categorical_columns = x_val.select_dtypes(include=['object', 'category']).columns\n","    encoder = joblib.load(\"onehot_encoder.pkl\")\n","\n","    if len(categorical_columns) > 0:\n","        x_val_encoded = encoder.transform(x_val[categorical_columns])\n","        encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n","        x_val_encoded_df = pd.DataFrame(x_val_encoded, columns=encoded_feature_names, index=x_val.index)\n","        x_val = x_val.drop(columns=categorical_columns)\n","        x_val = pd.concat([x_val, x_val_encoded_df], axis=1)\n","\n","    return x_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbCAdHq90VFz"},"outputs":[],"source":["# BILANCIAMENTO\n","def balance_data(x_train, y_train, target_count, num_datasets, random_seed):\n","    smote = BorderlineSMOTE(random_state=random_seed)\n","    oversampler = RandomOverSampler(random_state=random_seed)\n","\n","    class_counts = pd.Series(y_train).value_counts()\n","    smote_classes = [cls for cls in class_counts.index if class_counts[cls] < target_count / 2]\n","\n","    if smote_classes:\n","        smote_strategy = {cls: target_count for cls in smote_classes}\n","        smote = BorderlineSMOTE(sampling_strategy=smote_strategy, random_state=random_seed)\n","        x_train, y_train = smote.fit_resample(x_train, y_train)\n","        class_counts = pd.Series(y_train).value_counts()\n","\n","    over_classes = [cls for cls in class_counts.index if class_counts[cls] < target_count]\n","    if over_classes:\n","        over_strategy = {cls: target_count for cls in over_classes}\n","        oversampler = RandomOverSampler(sampling_strategy=over_strategy, random_state=random_seed)\n","        x_train, y_train = oversampler.fit_resample(x_train, y_train)\n","\n","    datasets = []\n","    for i in range(num_datasets):\n","        undersampler = RandomUnderSampler(sampling_strategy={cls: target_count for cls in pd.Series(y_train).value_counts().index}, random_state=random_seed + i)\n","        x_resampled, y_resampled = undersampler.fit_resample(x_train, y_train)\n","        x_resampled, y_resampled = shuffle(x_resampled, y_resampled, random_state=random_seed + i)\n","        datasets.append((x_resampled, y_resampled))\n","\n","\n","    return datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Atst5e5-0c_H"},"outputs":[],"source":["# PCA\n","def apply_pca_train(x_train, random_state, pca_threshold=0.99):\n","    pca = PCA(random_state=random_state)\n","    pca.fit(x_train)\n","    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n","    n_components = (cumulative_variance >= pca_threshold).argmax() + 1\n","    pca = PCA(n_components=n_components, random_state=random_state)\n","    transformed_data = pca.fit_transform(x_train)\n","    transformed_data = transformed_data.astype(np.float32)\n","\n","    print(f\"  Numero di colonne selezionate (componenti principali): {n_components}\")\n","    joblib.dump(pca, \"pca_model.pkl\")\n","    return pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n","\n","def apply_pca_validation(x_val):\n","    pca = joblib.load(\"pca_model.pkl\")\n","    x_val = pca.transform(x_val)\n","    x_val = x_val.astype(np.float32)\n","    return x_val\n","\n","# LDA\n","def apply_lda_train(x_train, y_train, lda_components=None):\n","    lda = LDA(n_components=lda_components)\n","    lda.fit(x_train, y_train)\n","    transformed_data = lda.transform(x_train)\n","    transformed_data = transformed_data.astype(np.float32)\n","\n","    n_components = transformed_data.shape[1]\n","    print(f\"  Numero di colonne selezionate (componenti discriminanti): {n_components}\")\n","    joblib.dump(lda, \"lda_model.pkl\")\n","    return pd.DataFrame(transformed_data, columns=[f\"LD{i+1}\" for i in range(n_components)])\n","\n","def apply_lda_validation(x_val):\n","    lda = joblib.load(\"lda_model.pkl\")\n","    x_val = lda.transform(x_val)\n","    x_val = x_val.astype(np.float32)\n","    return x_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YueC9OPp9afg"},"outputs":[],"source":["def preprocessing_pipeline(x_train, y_train, x_validation, y_validation, scaling_method, use_pca, pca_threshold, target_count=20000, num_datasets=1, random_seed=19):\n","    # Encoding delle feature\n","    x_train = encode_categorical_train_data(x_train)\n","    x_validation = encode_categorical_validation_data(x_validation)\n","\n","    # Bilanciamento\n","    datasets = balance_data(x_train, y_train, target_count, num_datasets, random_seed)\n","    validation = []\n","    data = []\n","    i = 0\n","\n","    for x_train, y_train in datasets:\n","      print(f\"  Dataset bilanciato {i+1}:\")\n","      i+=1\n","      # Scaling\n","      x_train, y_train = scale_train_data(x_train, y_train, scaling_method)\n","      x_val, y_val = scale_validation_data(x_validation, y_validation, scaling_method)\n","\n","      if use_pca == 'PCA':\n","          x_train = apply_pca_train(x_train, random_state=random_seed, pca_threshold=pca_threshold)\n","          x_val = apply_pca_validation(x_val)\n","      elif use_pca == 'LDA':\n","          x_train = apply_lda_train(x_train, y_train)\n","          x_val = apply_lda_validation(x_val)\n","\n","      # Bilanciamento\n","      data.append((x_train, y_train))\n","      validation.append((x_val, y_val))\n","    return data, validation\n"]},{"cell_type":"markdown","metadata":{"id":"lTUGMeKlHm1S"},"source":["# Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jez8IsOIKtHp"},"outputs":[],"source":["def fix_random(seed: int) -> None:\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True  # slower\n","\n","def is_combination_tested(filepath, new_row, num_epochs):\n","  \"\"\"\n","  Verifica se una combinazione Ã¨ giÃ  presente nel file CSV.\n","\n","  Parametri:\n","  - filepath (str): Percorso del file CSV.\n","  - new_row (dict): Dizionario con i valori da verificare.\n","\n","  Ritorna:\n","  - bool: True se la combinazione esiste, False altrimenti.\n","  \"\"\"\n","  return False\n","  # Leggi i risultati esistenti\n","  existing_results = pd.read_csv(filepath)\n","  comparison_columns = [\n","      'ds', 'random', 'outlier', 'pca', 'pca_threshold', 'scaler', 'target count',\n","      'batch_size', 'dim_embedding', 'num_heads', 'num_layers','learning_rate', 'new'\n","  ]\n","\n","  filtered_results = existing_results.copy()\n","  filtered_results = filtered_results[filtered_results['end'] == True] #solo combinazioni terminate\n","\n","  for col in comparison_columns:\n","    # Mantieni solo le righe in cui i valori corrispondono (o sono entrambi NaN)\n","    filtered_results = filtered_results[\n","        (filtered_results[col] == new_row[col]) | (pd.isna(filtered_results[col]) & pd.isna(new_row[col]))\n","    ]\n","\n","  # Controlla se tutte le colonne non in new_row sono NaN\n","  for _, row in filtered_results.iterrows():\n","    all_remaining_nan = all(pd.isna(row[col]) for col in comparison_columns if col not in new_row)\n","    if all_remaining_nan:\n","        print(\"  Configurazione giÃ  testata, salto...\")\n","        return True\n","\n","\n","def append_and_save_results(filepath, new_row, end=False):\n","  if not end:\n","      end = new_row['num_epochs'] == new_row['epoch']\n","  new_row['end']=end\n","  results_df = pd.read_csv(filepath)\n","  results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n","  results_df.to_csv(filepath, index=False)"]},{"cell_type":"code","source":["class MyDataset(Dataset):\n","    def __init__(self, X, y):\n","        if isinstance(X, (pd.DataFrame, pd.Series)):\n","            X = X.values\n","        if isinstance(y, (pd.DataFrame, pd.Series)):\n","            y = y.values\n","        self.X = torch.FloatTensor(X)\n","        self.y = torch.LongTensor(y)\n","\n","        self.num_features = X.shape[1]\n","        self.num_classes = len(np.unique(y))\n","\n","    def __len__(self):\n","        return self.X.shape[0]\n","\n","    def __getitem__(self, idx):\n","        return self.X[idx, :], self.y[idx]\n","\n","class TabTransformer(torch.nn.Module):\n","    def __init__(self, num_features, num_classes, dim_embedding=8, num_heads=2, num_layers=2):\n","        super(TabTransformer, self).__init__()\n","        self.embedding = torch.nn.Linear(num_features, dim_embedding)\n","        encoder_layer = torch.nn.TransformerEncoderLayer(d_model=dim_embedding, nhead=num_heads, batch_first=True)\n","        self.transformer = torch.nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n","        self.classifier = torch.nn.Linear(dim_embedding, num_classes)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        x = x.unsqueeze(1)\n","        x = self.transformer(x)\n","        x = torch.mean(x, dim=1)\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"Tq9-L4XRcuWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test_model(model, data_loader, device):\n","    model.eval()\n","    y_pred = []\n","    y_test = []\n","\n","    with torch.no_grad():\n","        for data, targets in data_loader:\n","            data, targets = data.to(device), targets.to(device)\n","            output = model(data)\n","            y_pred.append(output)\n","            y_test.append(targets)\n","\n","    y_test = torch.cat(y_test).squeeze()\n","    y_pred = torch.cat(y_pred).squeeze()\n","    y_pred_c = y_pred.argmax(dim=1, keepdim=True).squeeze()\n","\n","    return y_test, y_pred_c, y_pred\n","\n","\n","\n","def train_model(model, criterion, optimizer, epochs, data_loader, val_loader, device, scheduler, new_row):\n","    n_iter = 0\n","    best_model = None\n","    best_val_loss = float('inf')\n","    epochs_since_last_improvement = 0\n","    patience = new_row['patience']\n","\n","    start = time.time()\n","\n","    loss_history = []\n","    val_loss_history = []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","\n","        new_row['epoch'] = epoch+1\n","\n","        start_epoch = time.time()\n","        loss_train = 0\n","\n","        # Ciclo di training\n","        for data, targets in data_loader:\n","            # Trasferisci dati e target su GPU\n","            data, targets = data.to(device), targets.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(data)\n","            loss = criterion(outputs, targets.long())\n","            loss.backward()\n","            optimizer.step()\n","            n_iter += 1\n","            loss_train += loss.item()\n","\n","        scheduler.step()\n","        loss_train /= len(data_loader)\n","\n","        # Valutazione del modello\n","        model.eval()\n","        labels_list = []\n","        y_pred_c_list = []\n","        y_pred_list = []\n","\n","        with torch.no_grad():\n","            for val_data, val_targets in val_loader:\n","                # Trasferisci dati di validazione su GPU\n","                val_data, val_targets = val_data.to(device), val_targets.to(device)\n","                val_outputs = model(val_data)\n","                y_pred = torch.softmax(val_outputs, dim=1)\n","                y_pred_c = torch.argmax(y_pred, dim=1)\n","                labels_list.append(val_targets)\n","                y_pred_c_list.append(y_pred_c)\n","                y_pred_list.append(val_outputs)\n","\n","        # Concatenazione dei risultati di validazione\n","        labels = torch.cat(labels_list)\n","        y_pred_c = torch.cat(y_pred_c_list)\n","        y_pred = torch.cat(y_pred_list)\n","\n","        # Calcolo della loss e dell'accuratezza\n","        loss_val = criterion(y_pred, labels)\n","        val_accuracy = accuracy_score(labels.cpu(), y_pred_c.cpu())\n","\n","        # Report di classificazione\n","        report = classification_report(labels.cpu(), y_pred_c.cpu(), output_dict=True)\n","\n","        # Test accuracy, loss e report\n","        t_labels, t_pred_c, t_pred = test_model(model, data_loader, device)\n","        loss_t = criterion(t_pred, t_labels)\n","        t_report = classification_report(t_labels.cpu(), t_pred_c.cpu(), output_dict=True)\n","\n","        # Aggiornamento di `new_row` con i nuovi risultati\n","        new_row.update({\n","            'val_accuracy': val_accuracy,\n","            'val_precision': report['weighted avg']['precision'],\n","            'val_recall': report['weighted avg']['recall'],\n","            'val_f1': report['weighted avg']['f1-score'],\n","            'validation_loss': loss_val.item(),\n","            'train_accuracy': t_report['accuracy'],\n","            'train_precision': t_report['weighted avg']['precision'],\n","            'train_recall': t_report['weighted avg']['recall'],\n","            'train_f1': t_report['weighted avg']['f1-score'],\n","            'train_loss': loss_t.item()\n","        })\n","\n","        # Salvataggio del modello migliore\n","        if loss_val < best_val_loss:\n","            best_val_loss = loss_val\n","            best_model = model\n","            epochs_since_last_improvement = 0\n","        elif epochs_since_last_improvement >= patience:\n","            # Salva i risultati\n","            append_and_save_results(FILEPATH, new_row, True)\n","            print(f\"Early stopping at epoch {new_row['epoch']+1}\")\n","            break\n","        else:\n","            epochs_since_last_improvement += 1\n","\n","        # Salva i risultati\n","        append_and_save_results(FILEPATH, new_row)\n","\n","        loss_history.append(loss_train)\n","        val_loss_history.append(loss_val.item())\n","\n","        print(f'Epoch {epoch+1} - Val Loss: {loss_val:.6f} - Train Loss: {loss_train:.6f} - Patience: {epochs_since_last_improvement}')\n","        print(f\"  Validation Accuracy: {val_accuracy:.4f} - Train Accuracy: {t_report['accuracy']:.4f}\")\n","        current_time = time.time()\n","        time_difference = start_epoch - current_time\n","        print(f\"   Time: {time_difference / 60}\")\n","\n","    print(f'Training ended after {time.time() - start:.2f} seconds - Best Val Loss: {best_val_loss:.6f}')\n","\n","    return best_model, loss_history, val_loss_history"],"metadata":{"id":"7CXhd7jHc9mn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7O71MA0xtwq_"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","from itertools import product\n","import time\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","def trasf_with_grid(x_train, y_train, x_val, y_val, param_grid, metadata, random_state=19, scoring='accuracy'):\n","    \"\"\"\n","    Cerca i migliori iperparametri di una Random Forest valutando direttamente sul validation set.\n","    Salta configurazioni giÃ  testate.\n","    \"\"\"\n","    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    device = torch.device('cpu')\n","\n","    keys, values = zip(*param_grid.items())\n","    param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n","\n","    best_score = -float('inf')\n","    best_model = None\n","    best_report = None\n","\n","    # Encoding delle etichette\n","    le = LabelEncoder()\n","    y_train = le.fit_transform(y_train)\n","    y_val = le.transform(y_val)\n","    joblib.dump(le, \"label_encoder1.pkl\")\n","\n","    # Creazione dei dataset\n","    train_dataset = MyDataset(x_train, y_train)\n","    val_dataset = MyDataset(x_val, y_val)\n","\n","    # Calcolo dei pesi delle classi\n","    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n","    class_weights = dict(enumerate(class_weights))\n","\n","    # Trasferimento dei pesi delle classi sulla GPU\n","    class_weights_tensor = torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device)\n","\n","    num_feature = x_train.shape[1]\n","    num_classes = len(np.unique(y_train))\n","\n","    for params in param_combinations:\n","        fix_random(random_state)\n","        print(f\"Valutando configurazione: {params}\")\n","\n","        # Estrazione dei parametri dalla configurazione attuale\n","        epochs = params['num_epochs']\n","        batch_size = params['batch_size']\n","        dim_embedding = params['dim_embedding']\n","        num_heads = params['num_heads']\n","        num_layers = params['num_layers']\n","        lr = params['learning_rate']\n","\n","        # Crea una nuova riga di metadata\n","        new_row = metadata.copy()\n","        new_row.update(params)\n","        # Inizializzazione modello, loss e ottimizzatore\n","        criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor).to(device)\n","\n","        if not is_combination_tested(FILEPATH, new_row, epochs):\n","\n","            # Creazione dei dataloader\n","            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=(device.type == 'cpu'))\n","            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=(device.type == 'cpu'))\n","\n","            # Creazione del modello e trasferimento su GPU\n","            model = TabTransformer(num_feature, num_classes, dim_embedding, num_heads, num_layers).to(device)\n","            optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=params['step_size'], gamma=params['gamma'])\n","\n","            # Addestramento del modello\n","            model, loss_history, val_loss_history = train_model(\n","                model, criterion, optimizer, epochs, train_loader, val_loader, device, scheduler, new_row\n","            )\n","\n","    return best_model, best_score, best_report"]},{"cell_type":"code","source":["def apply(x_train, y_train, x_val, y_val, scaling_methods, param_grid,\n","  dim_reduction=['no'], pca_threshold=0.99, target_count=20000, num_datasets=1,\n","  random_seed=19, outs=['no'], info=''):\n","    \"\"\"\n","    Esegue l'intera pipeline di preprocessing, training e valutazione.\n","    \"\"\"\n","    results = {}\n","    x_train_fix = x_train\n","    x_val_fix = x_val\n","    bestbest = 0\n","\n","\n","    for dim_redx in dim_reduction:\n","      print(f\"\\n=== Testing Dimensionality Reduction: {dim_redx} ===\")\n","      if dim_redx != 'PCA':\n","        pca_threshold = None\n","      for scaling_method in scaling_methods:\n","        for out in outs:\n","            print(f\"\\n=== Testing Scaling Method: {scaling_method}, Outlier: {out} ===\")\n","\n","            x_train = replace_default_new(x_train_fix.copy(), info)\n","            x_val = apply_saved_modes(x_val_fix.copy(), info)\n","\n","            # Rimozione degli outlier\n","            x_train_filtered, y_train_filtered = remove_outliers(x_train, y_train, out)\n","\n","            # Preprocessing\n","            datasets, validation = preprocessing_pipeline(\n","                x_train_filtered, y_train_filtered, x_val, y_val,\n","                scaling_method, dim_redx, pca_threshold, target_count, num_datasets, random_seed\n","            )\n","\n","            results = {}\n","            for i, (x_train_processed, y_train_processed) in enumerate(datasets):\n","                    x_val_processed, y_val_processed = validation[i]\n","                    ds_name = f\"dataset_{i+1}\"\n","                    metadata = {\n","                        'ds': ds_name,\n","                        'random': random_seed,\n","                        'outlier': out,\n","                        'dim_reduction': dim_redx,\n","                        'pca_threshold': pca_threshold,\n","                        'scaler': scaling_method,\n","                        'target count': target_count,\n","                        'info': info\n","                    }\n","\n","                    print(f\"--- Training Dataset {i+1}/{len(datasets)} ---\")\n","                    best_model, best_score, best_report = trasf_with_grid(\n","                        x_train_processed, y_train_processed,\n","                        x_val_processed, y_val_processed,\n","                        param_grid, metadata, random_seed\n","                    )\n","                    if best_score > bestbest:\n","                      bestbest = best_score\n","                      joblib.dump(best_model, \"best_model.pkl\")\n","                      print(best_score)\n","                      print(best_report)\n","                    if best_model:\n","                        results[f\"{scaling_method}_dataset_{i+1}\"] = {\n","                            'best_model': best_model,\n","                            'best_score': best_score,\n","                            'classification_report': best_report # Store the report in results\n","                        }\n","                        joblib.dump(best_model, f\"best_model.pkl\")\n","                        print(best_score)\n","                        print(best_report)\n","    return results"],"metadata":{"id":"DNcq_g2mezns"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Run"],"metadata":{"id":"MzUMaQu8S97F"}},{"cell_type":"code","source":["scaling_methods = ['l1', 'l2']\n","out = ['no','base','isolation_forest', 'percentile',  'dynamic_threshold']\n","replace = ['no', 'mode']\n","\n","scaling_methods = ['l1']\n","out = ['no']\n","replace = ['mode']\n","\n","pca_threshold=0.99\n","dim_reduction = ['LDA', 'PCA']\n","\n","param_grid = {\n","    'num_epochs': [150],\n","    'batch_size': [16, 32, 64, 128, 256, 512],\n","    'patience': [30],\n","    'dim_embedding': [16, 32, 64, 128], #32\n","    'num_heads': [2, 4, 8],\n","    'num_layers': [2, 3, 4, 5],\n","    'learning_rate': [0.0009, 0.00095, 0.001, 0.0011, 0.01],\n","    'gamma': [0.3, 0.9],\n","    'step_size': [10, 15, 18, 20, 22]\n","}"],"metadata":{"id":"DH6HtBM0qN_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["scaling_methods = ['l1']\n","out = ['no']\n","replace = ['mode']\n","\n","pca_threshold=0.99\n","dim_reduction = ['LDA', 'PCA', 'no']\n","\n","param_grid = {\n","    'num_epochs': [250],\n","    'batch_size': [64],\n","    'patience': [30],\n","    'dim_embedding': [64], #32\n","    'num_heads': [2],\n","    'num_layers': [3],\n","    'learning_rate': [0.001],\n","    'gamma': [0.3],\n","    'step_size': [20]\n","}"],"metadata":{"id":"Pc10nDKKJvG7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# look for GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('mps')\n","print(\"Device: {}\".format(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAJMD904wnLv","executionInfo":{"status":"ok","timestamp":1738508512952,"user_tz":-60,"elapsed":12,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}},"outputId":"d0fd437e-a4cd-4771-a295-f147149d5038"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cpu\n"]}]},{"cell_type":"code","source":["# Esecuzione dell'esperimento\n","for r in replace:\n","  print(f\"\\n=== Testing Replace Value: {r} ===\")\n","  results = apply(\n","      X_train.copy(), y_train,\n","      X_val, y_val,\n","      scaling_methods, param_grid,\n","      dim_reduction=dim_reduction, pca_threshold=pca_threshold,\n","      target_count=20000, num_datasets=5, random_seed=19,\n","      outs = out,\n","      info = r\n","  )\n","  # Analisi dei risultati\n","  for key, value in results.items():\n","      print(f\"\\n=== Results for {key} ===\")\n","      print(f\"Best Model: {value['best_model']}\")\n","      print(\"Classification Report:\")\n","      print(value['classification_report'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UnXZWbxKNS-0","outputId":"675c7b2f-12cb-4a9b-b2c1-e50d1a0f7cc6","executionInfo":{"status":"error","timestamp":1738509227385,"user_tz":-60,"elapsed":714442,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Testing Replace Value: mode ===\n","\n","=== Testing Dimensionality Reduction: LDA ===\n","\n","=== Testing Scaling Method: l1, Outlier: no ===\n","  Dataset bilanciato 1:\n","  Numero di colonne selezionate (componenti discriminanti): 9\n","  Dataset bilanciato 2:\n","  Numero di colonne selezionate (componenti discriminanti): 9\n","  Dataset bilanciato 3:\n","  Numero di colonne selezionate (componenti discriminanti): 9\n","  Dataset bilanciato 4:\n","  Numero di colonne selezionate (componenti discriminanti): 9\n","  Dataset bilanciato 5:\n","  Numero di colonne selezionate (componenti discriminanti): 9\n","--- Training Dataset 1/5 ---\n","Valutando configurazione: {'num_epochs': 250, 'batch_size': 64, 'patience': 30, 'dim_embedding': 64, 'num_heads': 2, 'num_layers': 3, 'learning_rate': 0.001, 'gamma': 0.3, 'step_size': 20}\n","20 0.3\n","Epoch 1 - Val Loss: 0.241494 - Train Loss: 0.374273 - Patience: 0\n","  Validation Accuracy: 0.9191 - Train Accuracy: 0.9209\n","   Time: -1.8890013496081035\n","Epoch 2 - Val Loss: 0.197157 - Train Loss: 0.247511 - Patience: 0\n","  Validation Accuracy: 0.9466 - Train Accuracy: 0.9374\n","   Time: -1.987154229482015\n","Epoch 3 - Val Loss: 0.211345 - Train Loss: 0.206134 - Patience: 1\n","  Validation Accuracy: 0.9431 - Train Accuracy: 0.9344\n","   Time: -2.000482213497162\n","Epoch 4 - Val Loss: 0.154661 - Train Loss: 0.186887 - Patience: 0\n","  Validation Accuracy: 0.9588 - Train Accuracy: 0.9524\n","   Time: -2.152636404832204\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-198-b8588dacb36a>\", line 4, in <cell line: 0>\n","    results = apply(\n","              ^^^^^^\n","  File \"<ipython-input-194-13353e4766c9>\", line 49, in apply\n","    best_model, best_score, best_report = trasf_with_grid(\n","                                          ^^^^^^^^^^^^^^^^\n","  File \"<ipython-input-193-61140b18fc1e>\", line 74, in trasf_with_grid\n","    model, loss_history, val_loss_history = train_model(\n","                                            ^^^^^^^^^^^^\n","  File \"<ipython-input-192-f72520907a52>\", line 66, in train_model\n","    val_outputs = model(val_data)\n","                  ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"<ipython-input-191-f10dbc2c8c02>\", line 30, in forward\n","    x = self.transformer(x)\n","        ^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 511, in forward\n","    output = mod(\n","             ^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\", line 871, in forward\n","    return torch._transformer_encoder_layer_fwd(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","          ^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n","    traceback_info = getframeinfo(tb, context)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1688, in getframeinfo\n","    lines, lnum = findsource(frame)\n","                  ^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 170, in findsource\n","    file = getsourcefile(object) or getfile(object)\n","           ^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n","    module = getmodule(object, filename)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 992, in getmodule\n","    continue\n","KeyboardInterrupt\n"]},{"output_type":"error","ename":"TypeError","evalue":"object of type 'NoneType' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m<ipython-input-198-b8588dacb36a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n=== Testing Replace Value: {r} ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   results = apply(\n\u001b[0m\u001b[1;32m      5\u001b[0m       \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-194-13353e4766c9>\u001b[0m in \u001b[0;36mapply\u001b[0;34m(x_train, y_train, x_val, y_val, scaling_methods, param_grid, dim_reduction, pca_threshold, target_count, num_datasets, random_seed, outs, info)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"--- Training Dataset {i+1}/{len(datasets)} ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     best_model, best_score, best_report = trasf_with_grid(\n\u001b[0m\u001b[1;32m     50\u001b[0m                         \u001b[0mx_train_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_processed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-193-61140b18fc1e>\u001b[0m in \u001b[0;36mtrasf_with_grid\u001b[0;34m(x_train, y_train, x_val, y_val, param_grid, metadata, random_state, scoring)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;31m# Addestramento del modello\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             model, loss_history, val_loss_history = train_model(\n\u001b[0m\u001b[1;32m     75\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-192-f72520907a52>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, epochs, data_loader, val_loader, device, scheduler, new_row)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mval_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-191-f10dbc2c8c02>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m             output = mod(\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    870\u001b[0m                 )\n\u001b[0;32m--> 871\u001b[0;31m                 return torch._transformer_encoder_layer_fwd(\n\u001b[0m\u001b[1;32m    872\u001b[0m                     \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}]},{"cell_type":"code","source":["model = joblib.load(\"best_model_temp.pkl\")\n","print(model)"],"metadata":{"id":"fngDNmyjFMIb"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["oK4AXarlSXJf","EprNSrtG020W","xuYxMmVa9Yyh"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}