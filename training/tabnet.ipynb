{"cells":[{"cell_type":"code","source":["%%capture\n","!pip install pytorch-tabnet\n","#ELIMINAREEE"],"metadata":{"id":"qgDCpN2z_FaP","executionInfo":{"status":"ok","timestamp":1738589798952,"user_tz":-60,"elapsed":4342,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","execution_count":41,"metadata":{"id":"SydCnitAeLO_","executionInfo":{"status":"ok","timestamp":1738589800693,"user_tz":-60,"elapsed":1746,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["%%capture\n","import pandas as pd\n","import numpy as np\n","import warnings\n","from google.colab import drive\n","import ipaddress\n","import random\n","import os\n","\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch.utils.tensorboard import SummaryWriter\n","\n","\n","warnings.filterwarnings('ignore')\n","drive.mount('/content/drive')\n","\n","FILEPATH = \"/content/drive/MyDrive/data analytics/tabnet/results_0.csv\""]},{"cell_type":"markdown","metadata":{"id":"oK4AXarlSXJf"},"source":["# Creazione df result"]},{"cell_type":"code","source":["def create_empty_df(filepath):\n","  \"\"\"\n","  Crea un DataFrame vuoto e lo salva in un file CSV.\n","  \"\"\"\n","  # Definisci le colonne del DataFrame\n","  columns = [\n","      'ds', 'random', 'outlier', 'dim_reduction', 'pca_threshold', 'scaler', 'target count',\n","      'batch_size', 'dim_embedding', 'num_heads', 'num_layers', 'learning_rate', 'epoch',\n","      'gamma', 'step_size', 'weight_decay', 'info'\n","  ]\n","  # Crea un DataFrame vuoto\n","  results_df = pd.DataFrame(columns=columns)\n","\n","  # Salva il DataFrame in formato CSV\n","  results_df.to_csv(filepath, index=False)\n","\n","  print(f\"DataFrame creato e salvato in {filepath}\")\n","\n","create_empty_df(FILEPATH)"],"metadata":{"id":"_PR2nWm7RUOd","executionInfo":{"status":"ok","timestamp":1738589800694,"user_tz":-60,"elapsed":7,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f5adab99-2fa0-4e8d-ac41-07f8a585e24b"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["DataFrame creato e salvato in /content/drive/MyDrive/data analytics/tabnet/results_0.csv\n"]}]},{"cell_type":"markdown","metadata":{"id":"FJgi_sXUxr9c"},"source":["# Data Cleaning\n","Scelta colonne, cast delle colonne e gestione dei valori nulli\n","\n"]},{"cell_type":"code","execution_count":43,"metadata":{"id":"05w29vRUj2Q9","executionInfo":{"status":"ok","timestamp":1738589800694,"user_tz":-60,"elapsed":5,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["# Funzione per determinare il tipo di dato di una colonna\n","def type_data(column):\n","    default_val = [np.nan, '-']\n","    column = column[~column.isin(default_val)]\n","    unique_count = column.nunique()\n","    if is_binary_dtype(column):\n","        return 'Binario'\n","    if  is_numeric_dtype(column):\n","        return 'Numerico Discreto' if pd.api.types.is_integer_dtype(column) else 'Numerico Continuo'\n","    if is_category_dtype(column):\n","        return 'Categorico'\n","    return 'Unknown'\n","\n","# Funzioni ausiliarie per verificare il tipo di dato\n","def is_numeric_dtype(column):\n","    return pd.api.types.is_numeric_dtype(column)\n","\n","def is_binary_dtype(column):\n","    return set(column.unique()) == {True, False}\n","\n","def is_category_dtype(column):\n","    return pd.api.types.is_object_dtype(column) or pd.api.types.is_categorical_dtype(column)"]},{"cell_type":"code","execution_count":44,"metadata":{"id":"l-_JHS8nz5Uv","executionInfo":{"status":"ok","timestamp":1738589800694,"user_tz":-60,"elapsed":4,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["def clean_service_columns(data):\n","    service_related_cols = {}\n","    categorial_columns = data.select_dtypes(exclude=np.number).columns\n","    categorial_columns = categorial_columns.drop(['dns_qclass', 'dns_qtype', 'http_version', 'http_orig_mime_types', 'http_resp_mime_types'])\n","    for col in categorial_columns:\n","      for prefix in ['dns', 'http', 'ssl']:\n","        if col.startswith(prefix) and not pd.api.types.is_numeric_dtype(col):\n","          if prefix not in service_related_cols:\n","            service_related_cols[prefix] = []\n","          service_related_cols[prefix].append(col)\n","    for col in data.columns:\n","        for service, columns in service_related_cols.items():\n","            if col in columns and f\"service_{service}\" in data.columns:\n","                data.loc[~data[f\"service_{service}\"], col] = '/'\n","    return data\n","\n","def boolean_mapping(value, def_val=None):\n","    if value in {True, False}:\n","        return value\n","    if value == 'T':\n","        return True\n","    if value == 'F':\n","        return False\n","    return def_val if def_val is not None else value\n","\n","def categorize_ports(df, port_columns):\n","    port_bins = [0, 1023, 49151, 65535]\n","    port_labels = [\"Well-Known\", \"Registered\", \"Dynamic\"]\n","    for col in port_columns:\n","        df[col] = pd.cut(df[col], bins=port_bins, labels=port_labels, right=True)\n","    return df\n","\n","\n","def categorize_ip(ip):\n","    try:\n","        ip_obj = ipaddress.ip_address(ip)\n","        if ip_obj.is_loopback:\n","            return \"Loopback\"\n","        if ip_obj.is_private:\n","            return \"Private\"\n","        if ip_obj.is_multicast:\n","            return \"Multicast\"\n","        if ip_obj.is_reserved:\n","            return \"Reserved\"\n","        if ip_obj.is_link_local:\n","            return \"Link-Local\"\n","        return \"Public\"\n","    except ValueError:\n","        return \"Invalid\"\n","\n","def df_mapping(df):\n","  rcode_mapping = {0: 'No Error', 2: 'ServerFailure', 3: 'NameError', 5: 'Refuse'}\n","  qclass_mapping = {0: '-', 1: 'IN', 32769: 'CH'}\n","  qtype_mapping = {0: '-', 1: 'A', 2: 'NS', 5: 'CNAME', 28: 'AAAA', 255: 'ANY'}\n","\n","  for col in df.columns:\n","    if col in ['dns_RD', 'dns_AA', 'dns_rejected', 'http_trans_depth','ssl_established','ssl_resumed']:\n","      df[col] = df[col].map(lambda x: boolean_mapping(x,  def_val=False)).astype(str)\n","    if col in ['http_status_code', 'weird_addl', 'http_trans_depth']:\n","      df[col] = df[col].astype(str)\n","    if col == 'dns_qclass':\n","      df[col] = df[col].apply(lambda x: qclass_mapping.get(x, None))\n","    if col == 'dns_qtype':\n","      df[col] = df[col].apply(lambda x: qtype_mapping.get(x, None))\n","    if col == 'dns_rcode':\n","      df[col] = df[col].apply(lambda x: rcode_mapping.get(x, None))\n","    if col in ['src_ip', 'dst_ip']:\n","      df[col] = df[col].apply(categorize_ip)\n","    if col == 'src_bytes':\n","      df = df[df['src_bytes'] != '0.0.0.0']\n","      df['src_bytes'] = df['src_bytes'].astype(int)\n","  df = categorize_ports(df, ['src_port', 'dst_port'])\n","  return df\n","\n","def data_cleaning(df):\n","    services = df['service'].str.split(';').explode().unique()  # Estrazione di tutti i servizi unici\n","    for service in services:\n","        df[f'service_{service}'] = df['service'].apply(lambda x: service in x.split(';'))\n","\n","    df.drop(['http_referrer', 'service', 'service_-'], axis=1, inplace=True, errors='ignore')\n","    df.drop(['ts', 'ssl_subject', 'ssl_issuer', 'dns_query', 'http_uri', 'http_user_agent', 'weird_name', 'label'],\n","             axis=1, inplace=True, errors='ignore')\n","\n","    df = df_mapping(df)\n","    df = clean_service_columns(df)\n","\n","    return df"]},{"cell_type":"code","source":["def replace_default_new(df, info):\n","    mode_values = {}\n","    if info == 'mode' or info=='mode_all':\n","      for col in df.columns:\n","        if is_category_dtype(df[col]) or is_binary_dtype(df[col]):\n","            valid_values = df[(df[col] != '/') & (df[col] != '-')][col]\n","            mode_value = valid_values.mode()[0] if not valid_values.empty else '-'  # Usa '-' se non c'è moda\n","            mode_values[col] = mode_value\n","\n","            # Sostituzione valori\n","            df[col] = df[col].replace('-', mode_value)\n","            if info == 'mode_all':\n","              df[col] = df[col].replace('/', mode_value)\n","\n","    # Salva le mode con joblib\n","    joblib.dump(mode_values, \"mode.pk\")\n","\n","    return df\n","\n","\n","def apply_saved_modes(val, info):\n","    if info == 'mode' or info=='mode_all':\n","      mode_values = joblib.load(\"mode.pk\")\n","      # Applica le mode ai nuovi dati\n","      for col, mode_value in mode_values.items():\n","          if col in val.columns:\n","              val[col] = val[col].replace('-', mode_value)\n","              if info == 'mode_all':\n","                val[col] = val[col].replace('/', mode_value)\n","    return val"],"metadata":{"id":"qYX3qCVdyIyF","executionInfo":{"status":"ok","timestamp":1738589800694,"user_tz":-60,"elapsed":4,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","execution_count":46,"metadata":{"id":"d7d7wavGkHeZ","executionInfo":{"status":"ok","timestamp":1738589853391,"user_tz":-60,"elapsed":52701,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["df=pd.read_csv('/content/drive/MyDrive/data analytics/train_dataset.csv')\n","df = data_cleaning(df)"]},{"cell_type":"markdown","metadata":{"id":"EprNSrtG020W"},"source":["# Divisione val e train"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"ijcFM87B01a8","executionInfo":{"status":"ok","timestamp":1738589855264,"user_tz":-60,"elapsed":1876,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Definisci le features (X) e il target (y)\n","X = df.drop('type', axis=1)  # Assumi che 'label' sia la colonna del target\n","y = df['type']\n","\n","# Dividi il dataset in train e test set\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=19)\n","\n","# Unisci X_train e y_train\n","train_df = pd.concat([X_train, y_train], axis=1)\n","\n","# Unisci X_test e y_test\n","test_df = pd.concat([X_val, y_val], axis=1)"]},{"cell_type":"markdown","metadata":{"id":"xuYxMmVa9Yyh"},"source":["# Pipline"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"6yfSMEUnyn3Q","executionInfo":{"status":"ok","timestamp":1738589855266,"user_tz":-60,"elapsed":13,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, RobustScaler, QuantileTransformer, LabelEncoder, Normalizer\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.decomposition import PCA\n","from sklearn.utils import shuffle\n","import joblib\n","from imblearn.over_sampling import SMOTE,  BorderlineSMOTE, RandomOverSampler\n","from imblearn.under_sampling import RandomUnderSampler"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"hYW2GAR10g9D","executionInfo":{"status":"ok","timestamp":1738589855266,"user_tz":-60,"elapsed":12,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["#rimozione outlier per classe\n","def remove_outliers(x, y, out):\n","    x_train = x.copy()\n","    y_train = y.copy()\n","\n","    df = pd.concat([x_train, y_train], axis=1)\n","    numeric_cols = x_train.select_dtypes(include=np.number).columns\n","\n","    # Controlla se non deve essere applicata nessuna rimozione\n","    if out == 'no':\n","        return x, y\n","\n","    if out == 'base':\n","      #rimozione outlier piÃ¹ ASSURDI\n","      before = df.shape[0]\n","      df = df[df['duration'] < 1000]\n","      df = df[df['src_bytes']<100000000]\n","      df = df[df['dst_bytes']<100000000]\n","      df = df[df['missed_bytes']<100000000]\n","      df = df[df['src_pkts']<20000]\n","      df = df[df['dst_pkts']<20000]\n","      df = df[df['src_ip_bytes']<1000000]\n","      df = df[df['dst_ip_bytes']<1000000]\n","      print('  Rimosse ',before-df.shape[0],' istanze')\n","      x_train = df.drop('type', axis=1)\n","      y_train = df['type']\n","      return x_train, y_train\n","\n","    filtered_data = []\n","    # Itera su ciascuna classe\n","    for cls in df['type'].unique():\n","        class_df = df[df['type'] == cls]\n","        before = class_df.shape[0]\n","\n","        if out == 'iqr':\n","            for col in numeric_cols:\n","                Q1 = class_df[col].quantile(0.25)\n","                Q3 = class_df[col].quantile(0.75)\n","                IQR = Q3 - Q1\n","                lower_bound = Q1 - 1.5 * IQR\n","                upper_bound = Q3 + 1.5 * IQR\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        elif out == 'percentile':\n","            for col in numeric_cols:\n","                lower_bound = class_df[col].quantile(0.01)\n","                upper_bound = class_df[col].quantile(0.99)\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        elif out == 'isolation_forest':\n","            from sklearn.ensemble import IsolationForest\n","            iso = IsolationForest(contamination=0.05, random_state=19)\n","            numeric_data = class_df[numeric_cols]\n","            class_df['outlier'] = iso.fit_predict(numeric_data)\n","            class_df = class_df[class_df['outlier'] == 1].drop(columns=['outlier'])\n","\n","        elif out == 'dynamic_threshold':\n","            for col in numeric_cols:\n","                mean = class_df[col].mean()\n","                std = class_df[col].std()\n","                lower_bound = mean - 3 * std\n","                upper_bound = mean + 3 * std\n","                class_df = class_df[(class_df[col] >= lower_bound) & (class_df[col] <= upper_bound)]\n","\n","        filtered_data.append(class_df)\n","\n","    # Combina i dati filtrati per ciascuna classe\n","    filtered_df = pd.concat(filtered_data)\n","\n","    x_train = filtered_df.drop('type', axis=1)\n","    y_train = filtered_df['type']\n","\n","    return x_train, y_train"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"G1jZXJG_ysK9","executionInfo":{"status":"ok","timestamp":1738589855266,"user_tz":-60,"elapsed":12,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["# scaling and normalization\n","def scale_train_data(x_train, y_train, scaling_method):\n","    scaled_df = x_train.copy()\n","\n","    numeric_columns = x_train.select_dtypes(include=np.number).columns\n","    if len(numeric_columns) == 0:\n","        print(\"  Warning: No numeric columns to scale. Returning original DataFrame.\")\n","        return scaled_df, y_train\n","\n","    if scaling_method == 'none':\n","        print(\"No scaling applied.\")\n","        return scaled_df, y_train\n","    elif scaling_method == 'standard':\n","        scaler = StandardScaler()\n","    elif scaling_method == 'minmax':\n","        scaler = MinMaxScaler()\n","    elif scaling_method == 'quantile':\n","        scaled_df = pd.concat([scaled_df, y_train], axis=1)\n","        scaled_df = scaled_df.sort_values(by='src_bytes')\n","        y_train = scaled_df['type']\n","        scaled_df = scaled_df.drop('type', axis=1)\n","        scaler = QuantileTransformer(output_distribution='uniform', random_state=19)\n","    elif scaling_method == 'l1':\n","        scaler = Normalizer(norm='l1')\n","    elif scaling_method == 'l2':\n","        scaler = Normalizer(norm='l2')\n","    else:\n","        raise ValueError(f\"Metodo di scaling '{scaling_method}' non supportato.\")\n","\n","    if scaled_df[numeric_columns].shape[0] < 1:\n","        print(\"  Warning: Not enough samples to fit the scaler. Returning original DataFrame.\")\n","        return scaled_df, y_train\n","\n","    if scaling_method == 'l1' or scaling_method == 'l2':\n","        scaled_df = scaler.fit_transform(scaled_df)\n","    else:\n","        scaled_df[numeric_columns] = scaler.fit_transform(scaled_df[numeric_columns])\n","    joblib.dump(scaler, \"scaler.pkl\")\n","    return scaled_df, y_train\n","\n","# carica scaler e effettua scaling\n","def scale_validation_data(x_val, y_val, scaling_method):\n","    if scaling_method == 'quantile':\n","        x_val = pd.concat([x_val, y_val], axis=1)\n","        x_val = x_val.sort_values(by='src_bytes')\n","        y_val = x_val['type']\n","        x_val = x_val.drop('type', axis=1)\n","\n","    numeric_columns = x_val.select_dtypes(include=np.number).columns\n","    if scaling_method == 'none':\n","        print(\"No scaling applied to validation data.\")\n","        return x_val, y_val\n","\n","    scaler = joblib.load(\"scaler.pkl\")\n","\n","    if scaling_method == 'l1' or scaling_method == 'l2':\n","        x_val = scaler.transform(x_val)\n","    else:\n","        x_val[numeric_columns] = scaler.transform(x_val[numeric_columns])\n","    return x_val, y_val"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"92oPrwzabh3i","executionInfo":{"status":"ok","timestamp":1738589855266,"user_tz":-60,"elapsed":12,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["# ENCODING\n","def encode_categorical_train_data(x_train):\n","    categorical_columns = x_train.select_dtypes(include=['object', 'category']).columns\n","\n","    if len(categorical_columns) > 0:\n","        encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n","        encoder.fit(x_train[categorical_columns])\n","        joblib.dump(encoder, \"onehot_encoder.pkl\")\n","        x_train_encoded = encoder.transform(x_train[categorical_columns])\n","        encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n","        x_train_encoded_df = pd.DataFrame(x_train_encoded, columns=encoded_feature_names, index=x_train.index)\n","        x_train = x_train.drop(columns=categorical_columns)\n","        x_train = pd.concat([x_train, x_train_encoded_df], axis=1)\n","\n","    return x_train\n","\n","def encode_categorical_validation_data(x_val):\n","    categorical_columns = x_val.select_dtypes(include=['object', 'category']).columns\n","    encoder = joblib.load(\"onehot_encoder.pkl\")\n","\n","    if len(categorical_columns) > 0:\n","        x_val_encoded = encoder.transform(x_val[categorical_columns])\n","        encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n","        x_val_encoded_df = pd.DataFrame(x_val_encoded, columns=encoded_feature_names, index=x_val.index)\n","        x_val = x_val.drop(columns=categorical_columns)\n","        x_val = pd.concat([x_val, x_val_encoded_df], axis=1)\n","\n","    return x_val"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"vbCAdHq90VFz","executionInfo":{"status":"ok","timestamp":1738589855266,"user_tz":-60,"elapsed":11,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["# BILANCIAMENTO\n","def balance_data(x_train, y_train, target_count, num_datasets, random_seed):\n","    smote = BorderlineSMOTE(random_state=random_seed)\n","    oversampler = RandomOverSampler(random_state=random_seed)\n","\n","    class_counts = pd.Series(y_train).value_counts()\n","    smote_classes = [cls for cls in class_counts.index if class_counts[cls] < target_count / 2]\n","\n","    if smote_classes:\n","        smote_strategy = {cls: target_count for cls in smote_classes}\n","        smote = BorderlineSMOTE(sampling_strategy=smote_strategy, random_state=random_seed)\n","        x_train, y_train = smote.fit_resample(x_train, y_train)\n","        class_counts = pd.Series(y_train).value_counts()\n","\n","    over_classes = [cls for cls in class_counts.index if class_counts[cls] < target_count]\n","    if over_classes:\n","        over_strategy = {cls: target_count for cls in over_classes}\n","        oversampler = RandomOverSampler(sampling_strategy=over_strategy, random_state=random_seed)\n","        x_train, y_train = oversampler.fit_resample(x_train, y_train)\n","\n","    datasets = []\n","    for i in range(num_datasets):\n","        undersampler = RandomUnderSampler(sampling_strategy={cls: target_count for cls in pd.Series(y_train).value_counts().index}, random_state=random_seed + i)\n","        x_resampled, y_resampled = undersampler.fit_resample(x_train, y_train)\n","        x_resampled, y_resampled = shuffle(x_resampled, y_resampled, random_state=random_seed + i)\n","        datasets.append((x_resampled, y_resampled))\n","\n","\n","    return datasets"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"Atst5e5-0c_H","executionInfo":{"status":"ok","timestamp":1738589855266,"user_tz":-60,"elapsed":11,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["# PCA\n","def apply_pca_train(x_train, random_state, pca_threshold=0.99):\n","    pca = PCA(random_state=random_state)\n","    pca.fit(x_train)\n","    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n","    n_components = (cumulative_variance >= pca_threshold).argmax() + 1\n","    pca = PCA(n_components=n_components, random_state=random_state)\n","    transformed_data = pca.fit_transform(x_train)\n","    transformed_data = transformed_data.astype(np.float32)\n","\n","    print(f\"  Numero di colonne selezionate (componenti principali): {n_components}\")\n","    joblib.dump(pca, \"pca_model.pkl\")\n","    return pd.DataFrame(transformed_data, columns=[f\"PC{i+1}\" for i in range(n_components)])\n","\n","def apply_pca_validation(x_val):\n","    pca = joblib.load(\"pca_model.pkl\")\n","    x_val = pca.transform(x_val)\n","    x_val = x_val.astype(np.float32)\n","    return x_val\n","\n","# LDA\n","def apply_lda_train(x_train, y_train, lda_components=None):\n","    lda = LDA(n_components=lda_components)\n","    lda.fit(x_train, y_train)\n","    transformed_data = lda.transform(x_train)\n","    transformed_data = transformed_data.astype(np.float32)\n","\n","    n_components = transformed_data.shape[1]\n","    print(f\"  Numero di colonne selezionate (componenti discriminanti): {n_components}\")\n","    joblib.dump(lda, \"lda_model.pkl\")\n","    return pd.DataFrame(transformed_data, columns=[f\"LD{i+1}\" for i in range(n_components)])\n","\n","def apply_lda_validation(x_val):\n","    lda = joblib.load(\"lda_model.pkl\")\n","    x_val = lda.transform(x_val)\n","    x_val = x_val.astype(np.float32)\n","    return x_val"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"YueC9OPp9afg","executionInfo":{"status":"ok","timestamp":1738589855267,"user_tz":-60,"elapsed":11,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["def preprocessing_pipeline(x_train, y_train, x_validation, y_validation, scaling_method, use_pca, pca_threshold, target_count=20000, num_datasets=1, random_seed=19):\n","    # Encoding delle feature\n","    x_train = encode_categorical_train_data(x_train)\n","    x_validation = encode_categorical_validation_data(x_validation)\n","\n","    # Bilanciamento\n","    datasets = balance_data(x_train, y_train, target_count, num_datasets, random_seed)\n","    validation = []\n","    data = []\n","    i = 0\n","\n","    for x_train, y_train in datasets:\n","      print(f\"  Dataset bilanciato {i+1}:\")\n","      i+=1\n","      # Scaling\n","      x_train, y_train = scale_train_data(x_train, y_train, scaling_method)\n","      x_val, y_val = scale_validation_data(x_validation, y_validation, scaling_method)\n","\n","      if use_pca == 'PCA':\n","          x_train = apply_pca_train(x_train, random_state=random_seed, pca_threshold=pca_threshold)\n","          x_val = apply_pca_validation(x_val)\n","      elif use_pca == 'LDA':\n","          x_train = apply_lda_train(x_train, y_train)\n","          x_val = apply_lda_validation(x_val)\n","\n","      # Bilanciamento\n","      data.append((x_train, y_train))\n","      validation.append((x_val, y_val))\n","    return data, validation\n"]},{"cell_type":"markdown","metadata":{"id":"lTUGMeKlHm1S"},"source":["# Train"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"jez8IsOIKtHp","executionInfo":{"status":"ok","timestamp":1738589855267,"user_tz":-60,"elapsed":11,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["def fix_random(seed: int) -> None:\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True  # slower\n","\n","def is_combination_tested(filepath, new_row, num_epochs):\n","  \"\"\"\n","  Verifica se una combinazione è già presente nel file CSV.\n","\n","  Parametri:\n","  - filepath (str): Percorso del file CSV.\n","  - new_row (dict): Dizionario con i valori da verificare.\n","\n","  Ritorna:\n","  - bool: True se la combinazione esiste, False altrimenti.\n","  \"\"\"\n","  return False\n","  # Leggi i risultati esistenti\n","  existing_results = pd.read_csv(filepath)\n","  comparison_columns = [\n","      'ds', 'random', 'outlier', 'pca', 'pca_threshold', 'scaler', 'target count',\n","      'batch_size', 'dim_embedding', 'num_heads', 'num_layers','learning_rate', 'new'\n","  ]\n","\n","  filtered_results = existing_results.copy()\n","  filtered_results = filtered_results[filtered_results['end'] == True] #solo combinazioni terminate\n","\n","  for col in comparison_columns:\n","    # Mantieni solo le righe in cui i valori corrispondono (o sono entrambi NaN)\n","    filtered_results = filtered_results[\n","        (filtered_results[col] == new_row[col]) | (pd.isna(filtered_results[col]) & pd.isna(new_row[col]))\n","    ]\n","\n","  # Controlla se tutte le colonne non in new_row sono NaN\n","  for _, row in filtered_results.iterrows():\n","    all_remaining_nan = all(pd.isna(row[col]) for col in comparison_columns if col not in new_row)\n","    if all_remaining_nan:\n","        print(\"  Configurazione già testata, salto...\")\n","        return True\n","\n","def append_and_save_results(filepath, new_row):\n","  results_df = pd.read_csv(filepath)\n","  results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)\n","  results_df.to_csv(filepath, index=False)"]},{"cell_type":"code","source":["from pytorch_tabnet.tab_model import TabNetClassifier, TabNetRegressor\n","from pytorch_tabnet.pretraining import TabNetPretrainer\n","\n","class TabNet(torch.nn.Module):\n","    '''\n","    Wrapper class for TabNetClassifier\n","    '''\n","    def __init__(self, n_d,\n","                 n_a,\n","                 n_steps,\n","                 gamma,\n","                 optimizer_fn,\n","                 n_independent,\n","                 n_shared,\n","                 epsilon,\n","                 seed,\n","                 lambda_sparse,\n","                 clip_value,\n","                 momentum,\n","                 optimizer_params,\n","                 scheduler_params,\n","                 mask_type,\n","                 scheduler_fn,\n","                 device_name,\n","                 output_dim,\n","                 batch_size,\n","                 num_epochs,\n","                 unsupervised_model,\n","                 verbose=0):\n","        super(TabNet, self).__init__()\n","\n","        self.batch_size = batch_size\n","        self.num_epochs = num_epochs\n","        self.unsupervised_model = unsupervised_model\n","        self.network = TabNetClassifier(n_d=n_d,\n","                                        n_a=n_a,\n","                                        n_steps=n_steps,\n","                                        gamma=gamma,\n","                                        optimizer_fn=optimizer_fn,\n","                                        n_independent=n_independent,\n","                                        n_shared=n_shared,\n","                                        epsilon=epsilon,\n","                                        seed=seed,\n","                                        lambda_sparse=lambda_sparse,\n","                                        clip_value=clip_value,\n","                                        momentum=momentum,\n","                                        optimizer_params=optimizer_params,\n","                                        scheduler_params=scheduler_params,\n","                                        mask_type=mask_type,\n","                                        scheduler_fn=scheduler_fn,\n","                                        device_name=device_name,\n","                                        output_dim=output_dim,\n","                                        verbose=verbose)\n","\n","    def fit_model(self, X_train, y_train, X_val, y_val, criterion):\n","        self.network.fit(X_train=X_train,\n","                         y_train=y_train,\n","                         eval_set=[(X_train,y_train),(X_val, y_val)],\n","                         eval_metric=['accuracy'],\n","                         patience=10,\n","                         batch_size=self.batch_size,\n","                         virtual_batch_size=128,\n","                         num_workers=0,\n","                         drop_last=True,\n","                         max_epochs=self.num_epochs,\n","                         loss_fn=criterion,\n","                         from_unsupervised=self.unsupervised_model)\n","\n","    def predict(self, X):\n","        return self.network.predict(X)\n","\n","    def explain(self, X):\n","        return self.network.explain(X)\n","\n","    def feature_importances(self):\n","        return self.network.feature_importances_\n","\n","def get_unsupervised_model(n_d_a,n_step,n_independent,n_shared,gamma,lr):\n","    tabnet_params = dict(n_d=n_d_a,\n","                        n_a=n_d_a,\n","                        n_steps=n_step,\n","                        gamma=gamma,\n","                        n_independent=n_independent,\n","                        n_shared=n_shared,\n","                        lambda_sparse=1e-3,\n","                        optimizer_fn=torch.optim.AdamW,\n","                        optimizer_params=dict(lr=lr),\n","                        mask_type=\"sparsemax\",\n","                        verbose=0\n","                        )\n","    unsupervised_model = TabNetPretrainer(**tabnet_params)\n","    return unsupervised_model"],"metadata":{"id":"Tq9-L4XRcuWx","executionInfo":{"status":"ok","timestamp":1738589855267,"user_tz":-60,"elapsed":11,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","execution_count":63,"metadata":{"id":"7O71MA0xtwq_","executionInfo":{"status":"ok","timestamp":1738590226901,"user_tz":-60,"elapsed":244,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report, accuracy_score\n","from itertools import product\n","import time\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","\n","def trasf_with_grid(x_train, y_train, x_val, y_val, param_grid, metadata, random_state = 19, scoring='accuracy'):\n","  \"\"\"\n","  Cerca i migliori iperparametri di una Random Forest valutando direttamente sul validation set.\n","  Salta configurazioni già testate.\n","  \"\"\"\n","  keys, values = zip(*param_grid.items())\n","  param_combinations = [dict(zip(keys, v)) for v in product(*values)]\n","\n","  best_score = -float('inf')\n","  best_model = None\n","  best_report = None\n","\n","\n","  # Poi va salvato però\n","  le = LabelEncoder()\n","  y_train = le.fit_transform(y_train)  # Converte le categorie in interiù\n","  joblib.dump(le, \"label_encoder.pkl\")\n","  y_val = le.transform(y_val)          # Trasforma anche il validation set\n","\n","\n","  class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n","  class_weights = dict(enumerate(class_weights))\n","  print(class_weights)\n","\n","  num_feature = x_train.shape[1]\n","  num_classes = len(np.unique(y_train))\n","\n","  # Initialize the model, loss, and optimizer\n","  best_acc = -float('inf')\n","  best_loss = float('inf')\n","\n","  # per togliere i pesi modificare QUI\n","  criterion = torch.nn.CrossEntropyLoss(weight=torch.tensor(list(class_weights.values()), dtype=torch.float32).to(device))\n","  current_iter = 0\n","\n"," # Convert to numpy if not already a numpy array\n","  if not isinstance(x_train, np.ndarray):\n","    x_train = x_train.to_numpy()\n","  if not isinstance(x_val, np.ndarray):\n","    x_val = x_val.to_numpy()\n","\n","  for params in param_combinations:\n","      fix_random(random_state)\n","      print(f\"Valutando configurazione: {params}\")\n","\n","      # Estrai i parametri dalla configurazione attuale\n","      num_epochs = params['num_epochs']\n","      batch_size = params['batch_size']\n","      patience_ = params['patience']\n","      n_d = params['n_d_a']  # Numero di dimensioni nascoste per il decoder\n","      n_a = params['n_d_a']  # Numero di dimensioni nascoste per il decoder (uguale a n_d per TabNet)\n","      n_s = params['n_shared']  # Numero di layer condivisi\n","      n_i = params['n_indipendents']  # Numero di layer indipendenti\n","      n_steps_ = params['n_steps']  # Numero di steps TabNet\n","      gamma_ = params['gamma']  # Fattore gamma\n","      epsilon_ = params['epsilon']  # Valore di stabilità numerica\n","      lr = params['learning_rate']  # Learning rate\n","      pretraining_ratio_ = params['pretraining_ratio']  # Percentuale di dati per pretraining\n","      moment = params['momentum']  # Momento per l'ottimizzatore\n","\n","\n","      # Crea una nuova riga di metadata\n","      new_row = metadata.copy()\n","      new_row.update(params)\n","\n","\n","      # Verifica se la configurazione è già testata\n","      if True:#not is_combination_tested(FILEPATH, new_row):\n","\n","            # Addestra il modello\n","            print(f'Iteration {current_iter+1}')\n","            print(f'Hyperparameters: num_epochs={num_epochs}, batch_size={batch_size}, patience={patience_}, n_d={n_d}, n_indipendent={n_i}, n_shared={n_s}, n_steps={n_steps_}, gamma={gamma_}, epsilon={epsilon_}, lr={lr}, pretraining_ratio={pretraining_ratio_}, momentum={moment}')\n","\n","            unsupervised_model = get_unsupervised_model(n_d, n_steps_, n_i, n_s, gamma_, lr)\n","\n","            unsupervised_model.fit(\n","                X_train=x_train,\n","                eval_set=[x_val],\n","                max_epochs=num_epochs,\n","                patience=patience_,\n","                batch_size=batch_size,\n","                virtual_batch_size=128,\n","                num_workers=0,\n","                drop_last=False,\n","                pretraining_ratio=pretraining_ratio_,\n","            )\n","\n","            model = TabNet(n_d=n_d,\n","                          n_a=n_d,\n","                          n_steps=n_steps_,\n","                          gamma=gamma_,\n","                          optimizer_fn=torch.optim.AdamW,\n","                          n_independent=n_i,\n","                          n_shared=n_s,\n","                          epsilon=epsilon_,\n","                          seed=random_state,\n","                          lambda_sparse=1e-4,\n","                          clip_value=1,\n","                          momentum=moment,\n","                          optimizer_params=dict(lr=lr),\n","                          scheduler_params=dict(step_size=10, gamma=0.9),\n","                          mask_type='sparsemax',\n","                          scheduler_fn=torch.optim.lr_scheduler.StepLR,\n","                          device_name=device,\n","                          output_dim=len(np.unique(y_train)),\n","                          batch_size=batch_size,\n","                          num_epochs=num_epochs,\n","                          unsupervised_model=None,\n","                          verbose=0)\n","\n","            model.fit_model(x_train, y_train, x_val, y_val, criterion)\n","            y_pred = model.predict(x_val)\n","            acc = accuracy_score(y_val, y_pred)\n","            print(acc)\n","            new_row.update({\n","            'val_accuracy': acc})\n","\n","            # Salva i risultati\n","            #append_and_save_results(FILEPATH, new_row)\n","\n","\n","            if acc > best_acc:\n","                best_acc = acc\n","                best_model = model\n","                best_hyperparameters = f\"num_epochs={num_epochs}, batch_size={batch_size}, patience={patience_}, n_d={n_d}, n_indipendent={n_i}, n_shared={n_s}, n_steps={n_steps_}, gamma={gamma_}, epsilon={epsilon_}, lr={lr}, pretraining_ratio={pretraining_ratio_}, momentum={moment}\"\n","            current_iter += 1\n","\n","  return best_model, best_score"]},{"cell_type":"code","source":["def apply(x_train, y_train, x_val, y_val, scaling_methods, param_grid,\n","  dim_reduction=['no'], pca_threshold=0.99, target_count=20000, num_datasets=1,\n","  random_seed=19, outs=['no'], info=''):\n","    \"\"\"\n","    Esegue l'intera pipeline di preprocessing, training e valutazione.\n","    \"\"\"\n","    results = {}\n","    x_train_fix = x_train\n","    x_val_fix = x_val\n","    bestbest = 0\n","\n","\n","    for dim_redx in dim_reduction:\n","      print(f\"\\n=== Testing Dimensionality Reduction: {dim_redx} ===\")\n","      if dim_redx != 'PCA':\n","        pca_threshold = None\n","      for scaling_method in scaling_methods:\n","        for out in outs:\n","            print(f\"\\n=== Testing Scaling Method: {scaling_method}, Outlier: {out} ===\")\n","\n","            x_train = replace_default_new(x_train_fix.copy(), info)\n","            x_val = apply_saved_modes(x_val_fix.copy(), info)\n","\n","            # Rimozione degli outlier\n","            x_train_filtered, y_train_filtered = remove_outliers(x_train, y_train, out)\n","\n","            # Preprocessing\n","            datasets, validation = preprocessing_pipeline(\n","                x_train_filtered, y_train_filtered, x_val, y_val,\n","                scaling_method, dim_redx, pca_threshold, target_count, num_datasets, random_seed\n","            )\n","\n","            results = {}\n","            for i, (x_train_processed, y_train_processed) in enumerate(datasets):\n","                    x_val_processed, y_val_processed = validation[i]\n","                    ds_name = f\"dataset_{i+1}\"\n","                    metadata = {\n","                        'ds': ds_name,\n","                        'random': random_seed,\n","                        'outlier': out,\n","                        'dim_reduction': dim_redx,\n","                        'pca_threshold': pca_threshold,\n","                        'scaler': scaling_method,\n","                        'target count': target_count,\n","                        'info': info\n","                    }\n","\n","                    print(f\"--- Training Dataset {i+1}/{len(datasets)} ---\")\n","                    best_model, best_score = trasf_with_grid(\n","                        x_train_processed, y_train_processed,\n","                        x_val_processed, y_val_processed,\n","                        param_grid, metadata, random_seed\n","                    )\n","                    if best_model:\n","                        results[f\"{scaling_method}_dataset_{i+1}\"] = {\n","                            'best_model': best_model,\n","                            'best_score': best_score,\n","                        }\n","                        joblib.dump(best_model, f\"best_model.pkl\")\n","                        print(best_score)\n","    return results"],"metadata":{"id":"DNcq_g2mezns","executionInfo":{"status":"ok","timestamp":1738589855267,"user_tz":-60,"elapsed":10,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["#Run"],"metadata":{"id":"MzUMaQu8S97F"}},{"cell_type":"code","source":["scaling_methods = ['l1', 'l2']\n","out = ['no','base','isolation_forest', 'percentile',  'dynamic_threshold']\n","replace = ['no', 'mode']\n","\n","scaling_methods = ['l1']\n","out = ['no']\n","replace = ['mode']\n","\n","pca_threshold=0.99\n","dim_reduction = ['LDA', 'PCA']\n","\n","param_grid = {\n","    'num_epochs': [1],\n","    'batch_size': [512],\n","    'patience': [20],\n","    'n_d_a' : [128], # Dimensioni delle feature (decoder e encoder)\n","    'n_shared': [1], # Livelli condivisi tra le step layers\n","    'n_indipendents':[1], # Livelli indipendenti tra le step layers\n","    'n_steps' : [5],  # Numero di passi decisionali\n","    'gamma': [1.0],  # Peso del mascheramento tra i passi decisionali\n","    'epsilon' : [1e-15],  # Stabilità numerica\n","    'learning_rate': [0.001],\n","    'pretraining_ratio': [0.5],  # Percentuale dei dati usati per il pretraining\n","    'momentum' : [0.90],  # Momento per l'ottimizzazione\n","}"],"metadata":{"id":"DH6HtBM0qN_H","executionInfo":{"status":"ok","timestamp":1738589855267,"user_tz":-60,"elapsed":9,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["scaling_methods = ['l1']\n","out = ['no']\n","replace = ['mode']\n","\n","pca_threshold=0.99\n","dim_reduction = ['LDA', 'PCA', 'no']\n","\n","param_grid = {\n","    'num_epochs': [1],\n","    'batch_size': [512],\n","    'patience': [20],\n","    'n_d_a' : [128], # Dimensioni delle feature (decoder e encoder)\n","    'n_shared': [1], # Livelli condivisi tra le step layers\n","    'n_indipendents':[1], # Livelli indipendenti tra le step layers\n","    'n_steps' : [5],  # Numero di passi decisionali\n","    'gamma': [1.0],  # Peso del mascheramento tra i passi decisionali\n","    'epsilon' : [1e-15],  # Stabilità numerica\n","    'learning_rate': [0.001],\n","    'pretraining_ratio': [0.5],  # Percentuale dei dati usati per il pretraining\n","    'momentum' : [0.90],  # Momento per l'ottimizzazione\n","}"],"metadata":{"id":"Pc10nDKKJvG7","executionInfo":{"status":"ok","timestamp":1738589855267,"user_tz":-60,"elapsed":9,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["# look for GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","# device = torch.device('mps')\n","print(\"Device: {}\".format(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UAJMD904wnLv","executionInfo":{"status":"ok","timestamp":1738589855267,"user_tz":-60,"elapsed":9,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}},"outputId":"19e66bcd-ab8f-4a08-f266-435c47e13e93"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cpu\n"]}]},{"cell_type":"code","source":["# Esecuzione dell'esperimento\n","for r in replace:\n","  print(f\"\\n=== Testing Replace Value: {r} ===\")\n","  results = apply(\n","      X_train.copy(), y_train,\n","      X_val, y_val,\n","      scaling_methods, param_grid,\n","      dim_reduction=dim_reduction, pca_threshold=pca_threshold,\n","      target_count=20000, num_datasets=5, random_seed=19,\n","      outs = out,\n","      info = r\n","  )\n","  # Analisi dei risultati\n","  for key, value in results.items():\n","      print(f\"\\n=== Results for {key} ===\")\n","      print(f\"Best Model: {value['best_model']}\")\n","      print(\"Classification Report:\")\n","      print(value['classification_report'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UnXZWbxKNS-0","outputId":"7abc4960-b184-4ef2-9b2e-c49d5fc4b359","executionInfo":{"status":"error","timestamp":1738591428324,"user_tz":-60,"elapsed":1196049,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Testing Replace Value: mode ===\n","\n","=== Testing Dimensionality Reduction: LDA ===\n","\n","=== Testing Scaling Method: l1, Outlier: no ===\n","  Dataset bilanciato 1:\n","  Numero di colonne selezionate (componenti discriminanti): 9\n","  Dataset bilanciato 2:\n","  Numero di colonne selezionate (componenti discriminanti): 9\n","  Dataset bilanciato 3:\n","  Numero di colonne selezionate (componenti discriminanti): 9\n","  Dataset bilanciato 4:\n","  Numero di colonne selezionate (componenti discriminanti): 9\n","  Dataset bilanciato 5:\n","  Numero di colonne selezionate (componenti discriminanti): 9\n","--- Training Dataset 1/5 ---\n","{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0}\n","Valutando configurazione: {'num_epochs': 1, 'batch_size': 512, 'patience': 20, 'n_d_a': 128, 'n_shared': 1, 'n_indipendents': 1, 'n_steps': 5, 'gamma': 1.0, 'epsilon': 1e-15, 'learning_rate': 0.001, 'pretraining_ratio': 0.5, 'momentum': 0.9}\n","Iteration 1\n","Hyperparameters: num_epochs=1, batch_size=512, patience=20, n_d=128, n_indipendent=1, n_shared=1, n_steps=5, gamma=1.0, epsilon=1e-15, lr=0.001, pretraining_ratio=0.5, momentum=0.9\n","Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_unsup_loss_numpy = 1.5627700090408325\n","Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_1_accuracy = 0.90397\n","0.9039692701664532\n","-inf\n","--- Training Dataset 2/5 ---\n","{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0}\n","Valutando configurazione: {'num_epochs': 1, 'batch_size': 512, 'patience': 20, 'n_d_a': 128, 'n_shared': 1, 'n_indipendents': 1, 'n_steps': 5, 'gamma': 1.0, 'epsilon': 1e-15, 'learning_rate': 0.001, 'pretraining_ratio': 0.5, 'momentum': 0.9}\n","Iteration 1\n","Hyperparameters: num_epochs=1, batch_size=512, patience=20, n_d=128, n_indipendent=1, n_shared=1, n_steps=5, gamma=1.0, epsilon=1e-15, lr=0.001, pretraining_ratio=0.5, momentum=0.9\n","Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_unsup_loss_numpy = 1.6206899881362915\n","Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_1_accuracy = 0.92762\n","0.9276163308967731\n","-inf\n","--- Training Dataset 3/5 ---\n","{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0}\n","Valutando configurazione: {'num_epochs': 1, 'batch_size': 512, 'patience': 20, 'n_d_a': 128, 'n_shared': 1, 'n_indipendents': 1, 'n_steps': 5, 'gamma': 1.0, 'epsilon': 1e-15, 'learning_rate': 0.001, 'pretraining_ratio': 0.5, 'momentum': 0.9}\n","Iteration 1\n","Hyperparameters: num_epochs=1, batch_size=512, patience=20, n_d=128, n_indipendent=1, n_shared=1, n_steps=5, gamma=1.0, epsilon=1e-15, lr=0.001, pretraining_ratio=0.5, momentum=0.9\n","Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_unsup_loss_numpy = 1.6731499433517456\n","Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_1_accuracy = 0.92784\n","0.9278432389503882\n","-inf\n","--- Training Dataset 4/5 ---\n","{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0}\n","Valutando configurazione: {'num_epochs': 1, 'batch_size': 512, 'patience': 20, 'n_d_a': 128, 'n_shared': 1, 'n_indipendents': 1, 'n_steps': 5, 'gamma': 1.0, 'epsilon': 1e-15, 'learning_rate': 0.001, 'pretraining_ratio': 0.5, 'momentum': 0.9}\n","Iteration 1\n","Hyperparameters: num_epochs=1, batch_size=512, patience=20, n_d=128, n_indipendent=1, n_shared=1, n_steps=5, gamma=1.0, epsilon=1e-15, lr=0.001, pretraining_ratio=0.5, momentum=0.9\n","Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_unsup_loss_numpy = 1.2655800580978394\n","Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_1_accuracy = 0.89407\n","0.8940663543979643\n","-inf\n","--- Training Dataset 5/5 ---\n","{0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0, 6: 1.0, 7: 1.0, 8: 1.0, 9: 1.0}\n","Valutando configurazione: {'num_epochs': 1, 'batch_size': 512, 'patience': 20, 'n_d_a': 128, 'n_shared': 1, 'n_indipendents': 1, 'n_steps': 5, 'gamma': 1.0, 'epsilon': 1e-15, 'learning_rate': 0.001, 'pretraining_ratio': 0.5, 'momentum': 0.9}\n","Iteration 1\n","Hyperparameters: num_epochs=1, batch_size=512, patience=20, n_d=128, n_indipendent=1, n_shared=1, n_steps=5, gamma=1.0, epsilon=1e-15, lr=0.001, pretraining_ratio=0.5, momentum=0.9\n","Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_0_unsup_loss_numpy = 1.8698400259017944\n","Stop training because you reached max_epochs = 1 with best_epoch = 0 and best_val_1_accuracy = 0.92854\n","0.9285401708293489\n","-inf\n","\n","=== Testing Dimensionality Reduction: PCA ===\n","\n","=== Testing Scaling Method: l1, Outlier: no ===\n","  Dataset bilanciato 1:\n"]},{"output_type":"error","ename":"TypeError","evalue":"'>=' not supported between instances of 'float' and 'NoneType'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-64-b8588dacb36a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n=== Testing Replace Value: {r} ===\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   results = apply(\n\u001b[0m\u001b[1;32m      5\u001b[0m       \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-58-d05e28c329af>\u001b[0m in \u001b[0;36mapply\u001b[0;34m(x_train, y_train, x_val, y_val, scaling_methods, param_grid, dim_reduction, pca_threshold, target_count, num_datasets, random_seed, outs, info)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             datasets, validation = preprocessing_pipeline(\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mx_train_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_filtered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mscaling_method\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_redx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-54-4b208ab13e58>\u001b[0m in \u001b[0;36mpreprocessing_pipeline\u001b[0;34m(x_train, y_train, x_validation, y_validation, scaling_method, use_pca, pca_threshold, target_count, num_datasets, random_seed)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0muse_pca\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'PCA'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m           \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_pca_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpca_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m           \u001b[0mx_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_pca_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0muse_pca\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LDA'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-53-72649777924f>\u001b[0m in \u001b[0;36mapply_pca_train\u001b[0;34m(x_train, random_state, pca_threshold)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcumulative_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mn_components\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcumulative_variance\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mpca_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtransformed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: '>=' not supported between instances of 'float' and 'NoneType'"]}]},{"cell_type":"code","source":["model = joblib.load(\"best_model_temp.pkl\")\n","print(model)"],"metadata":{"id":"fngDNmyjFMIb","executionInfo":{"status":"aborted","timestamp":1738589998551,"user_tz":-60,"elapsed":6,"user":{"displayName":"Benedetta Bottari","userId":"06521195039600113604"}}},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["oK4AXarlSXJf","EprNSrtG020W","xuYxMmVa9Yyh"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}